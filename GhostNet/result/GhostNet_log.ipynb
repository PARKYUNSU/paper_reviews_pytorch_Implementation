{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/PARKYUNSU/GhostNet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T12:17:43.361774Z","iopub.execute_input":"2024-12-15T12:17:43.362340Z","iopub.status.idle":"2024-12-15T12:17:45.731891Z","shell.execute_reply.started":"2024-12-15T12:17:43.362294Z","shell.execute_reply":"2024-12-15T12:17:45.729268Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'GhostNet'...\nremote: Enumerating objects: 270, done.\u001b[K\nremote: Counting objects: 100% (89/89), done.\u001b[K\nremote: Compressing objects: 100% (64/64), done.\u001b[K\nremote: Total 270 (delta 53), reused 56 (delta 24), pack-reused 181 (from 1)\u001b[K\nReceiving objects: 100% (270/270), 25.76 MiB | 37.37 MiB/s, done.\nResolving deltas: 100% (146/146), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!python \"/kaggle/working/GhostNet/main.py\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T12:17:45.733520Z","iopub.execute_input":"2024-12-15T12:17:45.735927Z","iopub.status.idle":"2024-12-15T13:26:34.487867Z","shell.execute_reply.started":"2024-12-15T12:17:45.735883Z","shell.execute_reply":"2024-12-15T13:26:34.486927Z"}},"outputs":[{"name":"stdout","text":"=========================================================================================================\nLayer (type:depth-idx)                                  Output Shape              Param #\n=========================================================================================================\nGhostNet                                                [2, 80]                   --\n├─BasicConv: 1-1                                        [2, 16, 112, 112]         --\n│    └─Sequential: 2-1                                  [2, 16, 112, 112]         --\n│    │    └─Conv2d: 3-1                                 [2, 16, 112, 112]         432\n│    │    └─BatchNorm2d: 3-2                            [2, 16, 112, 112]         32\n│    │    └─ReLU: 3-3                                   [2, 16, 112, 112]         --\n├─Sequential: 1-2                                       [2, 160, 7, 7]            --\n│    └─GhostBottleneck: 2-2                             [2, 16, 112, 112]         --\n│    │    └─Sequential: 3-4                             [2, 16, 112, 112]         464\n│    └─GhostBottleneck: 2-3                             [2, 24, 56, 56]           --\n│    │    └─Sequential: 3-5                             [2, 24, 56, 56]           4,356\n│    │    └─Sequential: 3-6                             [2, 24, 56, 56]           896\n│    └─GhostBottleneck: 2-4                             [2, 24, 56, 56]           --\n│    │    └─Sequential: 3-7                             [2, 24, 56, 56]           2,352\n│    └─GhostBottleneck: 2-5                             [2, 40, 28, 28]           --\n│    │    └─Sequential: 3-8                             [2, 40, 28, 28]           11,744\n│    │    └─Sequential: 3-9                             [2, 40, 28, 28]           1,928\n│    └─GhostBottleneck: 2-6                             [2, 40, 28, 28]           --\n│    │    └─Sequential: 3-10                            [2, 40, 28, 28]           13,040\n│    └─GhostBottleneck: 2-7                             [2, 80, 14, 14]           --\n│    │    └─Sequential: 3-11                            [2, 80, 14, 14]           77,200\n│    │    └─Sequential: 3-12                            [2, 80, 14, 14]           5,480\n│    └─GhostBottleneck: 2-8                             [2, 80, 14, 14]           --\n│    │    └─Sequential: 3-13                            [2, 80, 14, 14]           17,820\n│    └─GhostBottleneck: 2-9                             [2, 80, 14, 14]           --\n│    │    └─Sequential: 3-14                            [2, 80, 14, 14]           16,436\n│    └─GhostBottleneck: 2-10                            [2, 80, 14, 14]           --\n│    │    └─Sequential: 3-15                            [2, 80, 14, 14]           16,436\n│    └─GhostBottleneck: 2-11                            [2, 112, 14, 14]          --\n│    │    └─Sequential: 3-16                            [2, 112, 14, 14]          165,128\n│    │    └─Sequential: 3-17                            [2, 112, 14, 14]          16,624\n│    └─GhostBottleneck: 2-12                            [2, 112, 14, 14]          --\n│    │    └─Sequential: 3-18                            [2, 112, 14, 14]          306,152\n│    └─GhostBottleneck: 2-13                            [2, 160, 7, 7]            --\n│    │    └─Sequential: 3-19                            [2, 160, 7, 7]            782,912\n│    │    └─Sequential: 3-20                            [2, 160, 7, 7]            32,240\n│    └─GhostBottleneck: 2-14                            [2, 160, 7, 7]            --\n│    │    └─Sequential: 3-21                            [2, 160, 7, 7]            160,880\n│    └─GhostBottleneck: 2-15                            [2, 160, 7, 7]            --\n│    │    └─Sequential: 3-22                            [2, 160, 7, 7]            621,680\n│    └─GhostBottleneck: 2-16                            [2, 160, 7, 7]            --\n│    │    └─Sequential: 3-23                            [2, 160, 7, 7]            160,880\n│    └─GhostBottleneck: 2-17                            [2, 160, 7, 7]            --\n│    │    └─Sequential: 3-24                            [2, 160, 7, 7]            621,680\n├─Sequential: 1-3                                       [2, 960, 1, 1]            --\n│    └─BasicConv: 2-18                                  [2, 960, 7, 7]            --\n│    │    └─Sequential: 3-25                            [2, 960, 7, 7]            155,520\n│    └─AdaptiveAvgPool2d: 2-19                          [2, 960, 1, 1]            --\n├─Sequential: 1-4                                       [2, 80]                   --\n│    └─Linear: 2-20                                     [2, 1280]                 1,228,800\n│    └─BatchNorm1d: 2-21                                [2, 1280]                 2,560\n│    └─ReLU: 2-22                                       [2, 1280]                 --\n│    └─Dropout: 2-23                                    [2, 1280]                 --\n│    └─Linear: 2-24                                     [2, 80]                   102,480\n=========================================================================================================\nTotal params: 4,526,152\nTrainable params: 4,526,152\nNon-trainable params: 0\nTotal mult-adds (M): 372.65\n=========================================================================================================\nInput size (MB): 1.20\nForward/backward pass size (MB): 125.66\nParams size (MB): 18.10\nEstimated Total Size (MB): 144.97\n=========================================================================================================\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n100%|███████████████████████| 170498071/170498071 [00:02<00:00, 79987618.13it/s]\nExtracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\nDataset: CIFAR-10\nBatch Size: 128, Learning Rate: 0.001, Weight Decay: 0.0001\n\nEpoch 1/30\nTrain Loss: 1.4653, Train Acc: 46.49%                                           \nVal Loss: 1.2992, Val Acc: 53.94%\nBest model saved!\n\nEpoch 2/30\nTrain Loss: 0.9974, Train Acc: 64.64%                                           \nVal Loss: 0.9535, Val Acc: 66.31%\nBest model saved!\n\nEpoch 3/30\nTrain Loss: 0.7954, Train Acc: 72.05%                                           \nVal Loss: 0.8616, Val Acc: 69.56%\nBest model saved!\n\nEpoch 4/30\nTrain Loss: 0.6489, Train Acc: 77.24%                                           \nVal Loss: 0.7578, Val Acc: 73.37%\nBest model saved!\n\nEpoch 5/30\nTrain Loss: 0.5403, Train Acc: 81.13%                                           \nVal Loss: 0.7693, Val Acc: 74.04%\nBest model saved!\n\nEpoch 6/30\nTrain Loss: 0.4692, Train Acc: 83.69%                                           \nVal Loss: 0.6175, Val Acc: 79.26%\nBest model saved!\n\nEpoch 7/30\nTrain Loss: 0.4043, Train Acc: 85.99%                                           \nVal Loss: 0.6015, Val Acc: 79.52%\nBest model saved!\n\nEpoch 8/30\nTrain Loss: 0.3512, Train Acc: 87.76%                                           \nVal Loss: 0.5469, Val Acc: 81.50%\nBest model saved!\n\nEpoch 9/30\nTrain Loss: 0.3195, Train Acc: 88.86%                                           \nVal Loss: 0.5668, Val Acc: 81.60%\nBest model saved!\n\nEpoch 10/30\nTrain Loss: 0.2808, Train Acc: 90.11%                                           \nVal Loss: 0.5457, Val Acc: 81.98%\nBest model saved!\n\nEpoch 11/30\nTrain Loss: 0.2455, Train Acc: 91.42%                                           \nVal Loss: 0.5662, Val Acc: 81.73%\n\nEpoch 12/30\nTrain Loss: 0.2252, Train Acc: 92.03%                                           \nVal Loss: 0.6014, Val Acc: 81.59%\n\nEpoch 13/30\nTrain Loss: 0.2102, Train Acc: 92.59%                                           \nVal Loss: 0.5496, Val Acc: 82.97%\nBest model saved!\n\nEpoch 14/30\nTrain Loss: 0.1855, Train Acc: 93.43%                                           \nVal Loss: 0.5557, Val Acc: 82.89%\n\nEpoch 15/30\nTrain Loss: 0.1652, Train Acc: 94.27%                                           \nVal Loss: 0.5868, Val Acc: 82.86%\n\nEpoch 16/30\nTrain Loss: 0.1636, Train Acc: 94.28%                                           \nVal Loss: 0.5825, Val Acc: 82.58%\n\nEpoch 17/30\nTrain Loss: 0.1475, Train Acc: 94.85%                                           \nVal Loss: 0.6037, Val Acc: 82.66%\n\nEpoch 18/30\nTrain Loss: 0.1371, Train Acc: 95.17%                                           \nVal Loss: 0.6029, Val Acc: 83.11%\nBest model saved!\n\nEpoch 19/30\nTrain Loss: 0.1332, Train Acc: 95.34%                                           \nVal Loss: 0.5771, Val Acc: 83.52%\nBest model saved!\n\nEpoch 20/30\nTrain Loss: 0.1218, Train Acc: 95.67%                                           \nVal Loss: 0.6024, Val Acc: 83.62%\nBest model saved!\n\nEpoch 21/30\nTrain Loss: 0.1138, Train Acc: 96.00%                                           \nVal Loss: 0.5822, Val Acc: 84.00%\nBest model saved!\n\nEpoch 22/30\nTrain Loss: 0.1136, Train Acc: 95.95%                                           \nVal Loss: 0.5989, Val Acc: 83.81%\n\nEpoch 23/30\nTrain Loss: 0.1143, Train Acc: 96.02%                                           \nVal Loss: 0.6051, Val Acc: 84.17%\nBest model saved!\n\nEpoch 24/30\nTrain Loss: 0.0976, Train Acc: 96.57%                                           \nVal Loss: 0.6194, Val Acc: 83.93%\n\nEpoch 25/30\nTrain Loss: 0.0995, Train Acc: 96.56%                                           \nVal Loss: 0.5900, Val Acc: 84.38%\nBest model saved!\n\nEpoch 26/30\nTrain Loss: 0.0981, Train Acc: 96.55%                                           \nVal Loss: 0.6009, Val Acc: 83.76%\n\nEpoch 27/30\nTrain Loss: 0.1003, Train Acc: 96.52%                                           \nVal Loss: 0.6215, Val Acc: 83.20%\n\nEpoch 28/30\nTrain Loss: 0.0851, Train Acc: 97.11%                                           \nVal Loss: 0.6276, Val Acc: 83.93%\n\nEpoch 29/30\nTrain Loss: 0.0843, Train Acc: 97.07%                                           \nVal Loss: 0.6351, Val Acc: 84.24%\n\nEpoch 30/30\nTrain Loss: 0.0899, Train Acc: 96.86%                                           \nVal Loss: 0.6133, Val Acc: 84.52%\nBest model saved!\n","output_type":"stream"}],"execution_count":2}]}