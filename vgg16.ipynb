{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2024-08-01T05:53:56.267488Z","iopub.status.busy":"2024-08-01T05:53:56.267071Z","iopub.status.idle":"2024-08-01T05:54:01.062887Z","shell.execute_reply":"2024-08-01T05:54:01.061918Z","shell.execute_reply.started":"2024-08-01T05:53:56.267456Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn # neural network\n","import torch.optim as optim\n","import torchvision\n","from torchvision import datasets # CIFAR10, CIFAR100\n","from torchvision import transforms # augmentation lib\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torch.utils.data import DataLoader\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import time\n","import os"]},{"cell_type":"code","execution_count":2,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2024-08-01T05:54:01.065669Z","iopub.status.busy":"2024-08-01T05:54:01.064682Z","iopub.status.idle":"2024-08-01T05:54:01.100608Z","shell.execute_reply":"2024-08-01T05:54:01.099671Z","shell.execute_reply.started":"2024-08-01T05:54:01.065629Z"},"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cuda')"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Device confg -> GPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"code","execution_count":3,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2024-08-01T05:54:01.102297Z","iopub.status.busy":"2024-08-01T05:54:01.101902Z","iopub.status.idle":"2024-08-01T05:54:01.113997Z","shell.execute_reply":"2024-08-01T05:54:01.113096Z","shell.execute_reply.started":"2024-08-01T05:54:01.102263Z"},"trusted":true},"outputs":[],"source":["# def data_loader(data_dir, batch_size, random_seed=42, shuffle=True, valid_size=0.1, test=False):\n","#     normalize = transforms.Normalize(\n","#         mean = [0.4914, 0.4822, 0.4465], # x - mean / std\n","#         std = [0.2023, 0.1994, 0.2010]\n","#     )\n","# # define transforms\n","#     transform = transforms.Compose([\n","#         transforms.Resize((224, 224)),\n","#         transforms.ToTensor(),\n","#         normalize\n","#     ])\n","\n","#     if test:\n","#         dataset = datasets.CIFAR10(\n","#             root = data_dir, train=False, download=True, transform=transform)\n","        \n","#         data_loader = torch.utils.data.DataLoader(\n","#             dataset, batch_size=batch_size, shuffle=shuffle\n","#         )\n","#         return data_loader\n","\n","# # load the dataset\n","#     train_dataset = datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n","#     valid_dataset = datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n","    \n","#     num_train = len(train_dataset)\n","#     indices = list(range(num_train))\n","#     split = int(np.floor(valid_size * num_train))\n","\n","#     if shuffle:\n","#         np.random.seed(random_seed)\n","#         np.random.shuffle(indices)\n","    \n","#     train_idx, valid_idx = indices[split:], indices[:split]\n","#     train_sampler = SubsetRandomSampler(train_idx)\n","#     valid_sampler = SubsetRandomSampler(valid_idx)\n","\n","#     train_loader = torch.utils.data.DataLoader(\n","#         train_dataset, batch_size=batch_size, sampler=train_sampler \n","#     )\n","    \n","#     valid_loader = torch.utils.data.DataLoader(\n","#         valid_dataset, batch_size=batch_size, sampler=valid_sampler\n","#     )\n","#     return (train_loader, valid_loader)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2024-08-01T05:54:01.118101Z","iopub.status.busy":"2024-08-01T05:54:01.117833Z","iopub.status.idle":"2024-08-01T05:54:01.128981Z","shell.execute_reply":"2024-08-01T05:54:01.128064Z","shell.execute_reply.started":"2024-08-01T05:54:01.118078Z"},"trusted":true},"outputs":[],"source":["def get_dataloaders_cifar10(batch_size, num_workers=0,\n","                            validation_fraction=None,\n","                            train_transforms=None,\n","                            test_transforms=None):\n","\n","    if train_transforms is None:\n","        train_transforms = transforms.ToTensor()\n","\n","    if test_transforms is None:\n","        test_transforms = transforms.ToTensor()\n","\n","    train_dataset = datasets.CIFAR10(root='/kaggle/working/data',\n","                                     train=True,\n","                                     transform=train_transforms,\n","                                     download=True)\n","\n","    valid_dataset = datasets.CIFAR10(root='/kaggle/working/data',\n","                                     train=True,\n","                                     transform=test_transforms)\n","\n","    test_dataset = datasets.CIFAR10(root='/kaggle/working/data',\n","                                    train=False,\n","                                    transform=test_transforms)\n","\n","    if validation_fraction is not None:\n","        num = int(validation_fraction * 50000)\n","        train_indices = torch.arange(0, 50000 - num)\n","        valid_indices = torch.arange(50000 - num, 50000)\n","\n","        train_sampler = SubsetRandomSampler(train_indices)\n","        valid_sampler = SubsetRandomSampler(valid_indices)\n","\n","        valid_loader = DataLoader(dataset=valid_dataset,\n","                                  batch_size=batch_size,\n","                                  num_workers=num_workers,\n","                                  sampler=valid_sampler)\n","\n","        train_loader = DataLoader(dataset=train_dataset,\n","                                  batch_size=batch_size,\n","                                  num_workers=num_workers,\n","                                  drop_last=True,\n","                                  sampler=train_sampler)\n","\n","    else:\n","        train_loader = DataLoader(dataset=train_dataset,\n","                                  batch_size=batch_size,\n","                                  num_workers=num_workers,\n","                                  drop_last=True,\n","                                  shuffle=True)\n","\n","    test_loader = DataLoader(dataset=test_dataset,\n","                             batch_size=batch_size,\n","                             num_workers=num_workers,\n","                             shuffle=False)\n","\n","    if validation_fraction is None:\n","        return train_loader, test_loader\n","    else:\n","        return train_loader, valid_loader, test_loader"]},{"cell_type":"code","execution_count":5,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2024-08-01T05:54:01.130881Z","iopub.status.busy":"2024-08-01T05:54:01.130102Z","iopub.status.idle":"2024-08-01T05:54:11.160068Z","shell.execute_reply":"2024-08-01T05:54:11.159153Z","shell.execute_reply.started":"2024-08-01T05:54:01.130854Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /kaggle/working/data/cifar-10-python.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 170498071/170498071 [00:06<00:00, 28284330.84it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting /kaggle/working/data/cifar-10-python.tar.gz to /kaggle/working/data\n"]}],"source":["train_transforms = torchvision.transforms.Compose([\n","    torchvision.transforms.RandomHorizontalFlip(),  # 랜덤 수평 뒤집기\n","    torchvision.transforms.RandomCrop(32, padding=4),  # 랜덤 크롭 (패딩 포함)\n","    torchvision.transforms.Resize((224, 224)),\n","    torchvision.transforms.ToTensor(),\n","    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","test_transforms = torchvision.transforms.Compose([\n","    torchvision.transforms.Resize((224, 224)),\n","    torchvision.transforms.ToTensor(),                \n","    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])\n","\n","train_loader, valid_loader, test_loader = get_dataloaders_cifar10(\n","    batch_size=64,\n","    validation_fraction=0.1,\n","    train_transforms=train_transforms,\n","    test_transforms=test_transforms,\n","    num_workers=2)"]},{"cell_type":"code","execution_count":6,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2024-08-01T05:54:11.162248Z","iopub.status.busy":"2024-08-01T05:54:11.161480Z","iopub.status.idle":"2024-08-01T05:54:11.166255Z","shell.execute_reply":"2024-08-01T05:54:11.165354Z","shell.execute_reply.started":"2024-08-01T05:54:11.162214Z"},"trusted":true},"outputs":[],"source":["# train_loader, valid_loader = data_loader(data_dir='/kaggle/working/data', batch_size=64)"]},{"cell_type":"code","execution_count":7,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2024-08-01T05:54:11.168103Z","iopub.status.busy":"2024-08-01T05:54:11.167669Z","iopub.status.idle":"2024-08-01T05:54:11.175651Z","shell.execute_reply":"2024-08-01T05:54:11.174849Z","shell.execute_reply.started":"2024-08-01T05:54:11.168067Z"},"trusted":true},"outputs":[],"source":["# test_loader = data_loader(data_dir='/kaggle/working/data', batch_size=64, test=True)"]},{"cell_type":"code","execution_count":8,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2024-08-01T05:54:11.177539Z","iopub.status.busy":"2024-08-01T05:54:11.177262Z","iopub.status.idle":"2024-08-01T05:54:11.201018Z","shell.execute_reply":"2024-08-01T05:54:11.200183Z","shell.execute_reply.started":"2024-08-01T05:54:11.177516Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class VGG16(nn.Module):\n","    def __init__(self, num_classes=100):\n","        super(VGG16, self).__init__()\n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU()\n","        )\n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","\n","        self.layer3 = nn.Sequential(\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU()\n","        )\n","        self.layer4 = nn.Sequential(\n","            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","\n","        self.layer5 = nn.Sequential(\n","            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU()\n","        )\n","        self.layer6 = nn.Sequential(\n","            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU()\n","        )\n","        self.layer7 = nn.Sequential(\n","            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","\n","        self.layer8 = nn.Sequential(\n","            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU()\n","        )\n","        self.layer9 = nn.Sequential(\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU()\n","        )\n","        self.layer10 = nn.Sequential(\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","\n","        self.layer11 = nn.Sequential(\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU()\n","        )\n","        self.layer12 = nn.Sequential(\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU()\n","        )\n","        self.layer13 = nn.Sequential(\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","\n","        # Fully connected layer\n","        self.fc = nn.Sequential(\n","            nn.Linear(7*7*512, 4096),\n","            nn.ReLU(),\n","            nn.Dropout(0.5)\n","        )\n","        self.fc1 = nn.Sequential(\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(),\n","            nn.Dropout(0.5)\n","        )\n","        self.fc2 = nn.Sequential(\n","            nn.Linear(4096, num_classes)\n","        )\n","        \n","        for m in self.modules():\n","            if isinstance(m, torch.torch.nn.Conv2d) or isinstance(m, torch.torch.nn.Linear):\n","                torch.nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n","                if m.bias is not None:\n","                    m.bias.detach().zero_()\n","                    \n","        self.avgpool = torch.nn.AdaptiveAvgPool2d((7, 7))        \n","\n","    def forward(self, x):\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","        x = self.layer5(x)\n","        x = self.layer6(x)\n","        x = self.layer7(x)\n","        x = self.layer8(x)\n","        x = self.layer9(x)\n","        x = self.layer10(x)\n","        x = self.layer11(x)\n","        x = self.layer12(x)\n","        x = self.layer13(x)\n","        x = self.avgpool(x)\n","        x = x.reshape(x.size(0), -1)  # flatten (batch_size, num_channels * height * width)\n","        x = self.fc(x)\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        return x"]},{"cell_type":"code","execution_count":9,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2024-08-01T05:54:11.202574Z","iopub.status.busy":"2024-08-01T05:54:11.202285Z","iopub.status.idle":"2024-08-01T05:54:27.614623Z","shell.execute_reply":"2024-08-01T05:54:27.613560Z","shell.execute_reply.started":"2024-08-01T05:54:11.202551Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torchinfo in /opt/conda/lib/python3.10/site-packages (1.8.0)\n"]},{"data":{"text/plain":["===================================================================================================================\n","Layer (type:depth-idx)                   Input Shape               Output Shape              Param #\n","===================================================================================================================\n","VGG16                                    [1, 3, 224, 224]          [1, 10]                   --\n","├─Sequential: 1-1                        [1, 3, 224, 224]          [1, 64, 224, 224]         --\n","│    └─Conv2d: 2-1                       [1, 3, 224, 224]          [1, 64, 224, 224]         1,792\n","│    └─BatchNorm2d: 2-2                  [1, 64, 224, 224]         [1, 64, 224, 224]         128\n","│    └─ReLU: 2-3                         [1, 64, 224, 224]         [1, 64, 224, 224]         --\n","├─Sequential: 1-2                        [1, 64, 224, 224]         [1, 64, 112, 112]         --\n","│    └─Conv2d: 2-4                       [1, 64, 224, 224]         [1, 64, 224, 224]         36,928\n","│    └─BatchNorm2d: 2-5                  [1, 64, 224, 224]         [1, 64, 224, 224]         128\n","│    └─ReLU: 2-6                         [1, 64, 224, 224]         [1, 64, 224, 224]         --\n","│    └─MaxPool2d: 2-7                    [1, 64, 224, 224]         [1, 64, 112, 112]         --\n","├─Sequential: 1-3                        [1, 64, 112, 112]         [1, 128, 112, 112]        --\n","│    └─Conv2d: 2-8                       [1, 64, 112, 112]         [1, 128, 112, 112]        73,856\n","│    └─BatchNorm2d: 2-9                  [1, 128, 112, 112]        [1, 128, 112, 112]        256\n","│    └─ReLU: 2-10                        [1, 128, 112, 112]        [1, 128, 112, 112]        --\n","├─Sequential: 1-4                        [1, 128, 112, 112]        [1, 128, 56, 56]          --\n","│    └─Conv2d: 2-11                      [1, 128, 112, 112]        [1, 128, 112, 112]        147,584\n","│    └─BatchNorm2d: 2-12                 [1, 128, 112, 112]        [1, 128, 112, 112]        256\n","│    └─ReLU: 2-13                        [1, 128, 112, 112]        [1, 128, 112, 112]        --\n","│    └─MaxPool2d: 2-14                   [1, 128, 112, 112]        [1, 128, 56, 56]          --\n","├─Sequential: 1-5                        [1, 128, 56, 56]          [1, 256, 56, 56]          --\n","│    └─Conv2d: 2-15                      [1, 128, 56, 56]          [1, 256, 56, 56]          295,168\n","│    └─BatchNorm2d: 2-16                 [1, 256, 56, 56]          [1, 256, 56, 56]          512\n","│    └─ReLU: 2-17                        [1, 256, 56, 56]          [1, 256, 56, 56]          --\n","├─Sequential: 1-6                        [1, 256, 56, 56]          [1, 256, 56, 56]          --\n","│    └─Conv2d: 2-18                      [1, 256, 56, 56]          [1, 256, 56, 56]          590,080\n","│    └─BatchNorm2d: 2-19                 [1, 256, 56, 56]          [1, 256, 56, 56]          512\n","│    └─ReLU: 2-20                        [1, 256, 56, 56]          [1, 256, 56, 56]          --\n","├─Sequential: 1-7                        [1, 256, 56, 56]          [1, 256, 28, 28]          --\n","│    └─Conv2d: 2-21                      [1, 256, 56, 56]          [1, 256, 56, 56]          590,080\n","│    └─BatchNorm2d: 2-22                 [1, 256, 56, 56]          [1, 256, 56, 56]          512\n","│    └─ReLU: 2-23                        [1, 256, 56, 56]          [1, 256, 56, 56]          --\n","│    └─MaxPool2d: 2-24                   [1, 256, 56, 56]          [1, 256, 28, 28]          --\n","├─Sequential: 1-8                        [1, 256, 28, 28]          [1, 512, 28, 28]          --\n","│    └─Conv2d: 2-25                      [1, 256, 28, 28]          [1, 512, 28, 28]          1,180,160\n","│    └─BatchNorm2d: 2-26                 [1, 512, 28, 28]          [1, 512, 28, 28]          1,024\n","│    └─ReLU: 2-27                        [1, 512, 28, 28]          [1, 512, 28, 28]          --\n","├─Sequential: 1-9                        [1, 512, 28, 28]          [1, 512, 28, 28]          --\n","│    └─Conv2d: 2-28                      [1, 512, 28, 28]          [1, 512, 28, 28]          2,359,808\n","│    └─BatchNorm2d: 2-29                 [1, 512, 28, 28]          [1, 512, 28, 28]          1,024\n","│    └─ReLU: 2-30                        [1, 512, 28, 28]          [1, 512, 28, 28]          --\n","├─Sequential: 1-10                       [1, 512, 28, 28]          [1, 512, 14, 14]          --\n","│    └─Conv2d: 2-31                      [1, 512, 28, 28]          [1, 512, 28, 28]          2,359,808\n","│    └─BatchNorm2d: 2-32                 [1, 512, 28, 28]          [1, 512, 28, 28]          1,024\n","│    └─ReLU: 2-33                        [1, 512, 28, 28]          [1, 512, 28, 28]          --\n","│    └─MaxPool2d: 2-34                   [1, 512, 28, 28]          [1, 512, 14, 14]          --\n","├─Sequential: 1-11                       [1, 512, 14, 14]          [1, 512, 14, 14]          --\n","│    └─Conv2d: 2-35                      [1, 512, 14, 14]          [1, 512, 14, 14]          2,359,808\n","│    └─BatchNorm2d: 2-36                 [1, 512, 14, 14]          [1, 512, 14, 14]          1,024\n","│    └─ReLU: 2-37                        [1, 512, 14, 14]          [1, 512, 14, 14]          --\n","├─Sequential: 1-12                       [1, 512, 14, 14]          [1, 512, 14, 14]          --\n","│    └─Conv2d: 2-38                      [1, 512, 14, 14]          [1, 512, 14, 14]          2,359,808\n","│    └─BatchNorm2d: 2-39                 [1, 512, 14, 14]          [1, 512, 14, 14]          1,024\n","│    └─ReLU: 2-40                        [1, 512, 14, 14]          [1, 512, 14, 14]          --\n","├─Sequential: 1-13                       [1, 512, 14, 14]          [1, 512, 7, 7]            --\n","│    └─Conv2d: 2-41                      [1, 512, 14, 14]          [1, 512, 14, 14]          2,359,808\n","│    └─BatchNorm2d: 2-42                 [1, 512, 14, 14]          [1, 512, 14, 14]          1,024\n","│    └─ReLU: 2-43                        [1, 512, 14, 14]          [1, 512, 14, 14]          --\n","│    └─MaxPool2d: 2-44                   [1, 512, 14, 14]          [1, 512, 7, 7]            --\n","├─AdaptiveAvgPool2d: 1-14                [1, 512, 7, 7]            [1, 512, 7, 7]            --\n","├─Sequential: 1-15                       [1, 25088]                [1, 4096]                 --\n","│    └─Linear: 2-45                      [1, 25088]                [1, 4096]                 102,764,544\n","│    └─ReLU: 2-46                        [1, 4096]                 [1, 4096]                 --\n","│    └─Dropout: 2-47                     [1, 4096]                 [1, 4096]                 --\n","├─Sequential: 1-16                       [1, 4096]                 [1, 4096]                 --\n","│    └─Linear: 2-48                      [1, 4096]                 [1, 4096]                 16,781,312\n","│    └─ReLU: 2-49                        [1, 4096]                 [1, 4096]                 --\n","│    └─Dropout: 2-50                     [1, 4096]                 [1, 4096]                 --\n","├─Sequential: 1-17                       [1, 4096]                 [1, 10]                   --\n","│    └─Linear: 2-51                      [1, 4096]                 [1, 10]                   40,970\n","===================================================================================================================\n","Total params: 134,309,962\n","Trainable params: 134,309,962\n","Non-trainable params: 0\n","Total mult-adds (G): 15.48\n","===================================================================================================================\n","Input size (MB): 0.60\n","Forward/backward pass size (MB): 216.83\n","Params size (MB): 537.24\n","Estimated Total Size (MB): 754.67\n","==================================================================================================================="]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["!pip install torchinfo  # Install the missing module\n","from torchinfo import summary  # Import after installation\n","\n","# Define a sample input tensor\n","features = torch.randn(1, 3, 224, 224)  # Assuming your model expects input of shape (batch, channels, height, width)\n","\n","# Move the model and input to the same device (if using GPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = VGG16(num_classes=10).to(device)\n","model = model.to(device)\n","features = features.to(device)\n","\n","summary(model, input_data=features, col_names=(\"input_size\", \"output_size\", \"num_params\"))  # Now this should work"]},{"cell_type":"code","execution_count":10,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2024-08-01T05:54:27.618239Z","iopub.status.busy":"2024-08-01T05:54:27.617936Z","iopub.status.idle":"2024-08-01T05:54:27.624311Z","shell.execute_reply":"2024-08-01T05:54:27.623373Z","shell.execute_reply.started":"2024-08-01T05:54:27.618214Z"},"trusted":true},"outputs":[{"data":{"text/plain":["703"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["total_step = len(train_loader)\n","total_step"]},{"cell_type":"code","execution_count":20,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2024-08-01T10:39:28.848033Z","iopub.status.busy":"2024-08-01T10:39:28.847243Z","iopub.status.idle":"2024-08-01T10:39:28.860888Z","shell.execute_reply":"2024-08-01T10:39:28.859955Z","shell.execute_reply.started":"2024-08-01T10:39:28.847995Z"},"trusted":true},"outputs":[],"source":["def plot_training_loss(minibatch_loss_list, num_epochs, iter_per_epoch,\n","                       results_dir=None, averaging_iterations=100):\n","\n","    plt.figure()\n","    ax1 = plt.subplot(1, 1, 1)\n","    ax1.plot(range(len(minibatch_loss_list)),\n","             (minibatch_loss_list), label='Minibatch Loss')\n","\n","    if len(minibatch_loss_list) > 1000:\n","        ax1.set_ylim([\n","            0, np.max(minibatch_loss_list[1000:])*1.5\n","            ])\n","    ax1.set_xlabel('Iterations')\n","    ax1.set_ylabel('Loss')\n","\n","    ax1.plot(np.convolve(minibatch_loss_list,\n","                         np.ones(averaging_iterations,)/averaging_iterations,\n","                         mode='valid'),\n","             label='Running Average')\n","    ax1.legend()\n","\n","    ###################\n","    # Set scond x-axis\n","    ax2 = ax1.twiny()\n","    newlabel = list(range(num_epochs+1))\n","\n","    newpos = [e*iter_per_epoch for e in newlabel]\n","\n","    ax2.set_xticks(newpos[::10])\n","    ax2.set_xticklabels(newlabel[::10])\n","\n","    ax2.xaxis.set_ticks_position('bottom')\n","    ax2.xaxis.set_label_position('bottom')\n","    ax2.spines['bottom'].set_position(('outward', 45))\n","    ax2.set_xlabel('Epochs')\n","    ax2.set_xlim(ax1.get_xlim())\n","    ###################\n","\n","    plt.tight_layout()\n","\n","    if results_dir is not None:\n","        image_path = os.path.join(results_dir, '/kaggle/working/data/plot_training_loss.pdf')\n","        plt.savefig(image_path)\n","\n","def plot_accuracy(train_acc_list, valid_acc_list, results_dir):\n","\n","    num_epochs = len(train_acc_list)\n","\n","    plt.plot(np.arange(1, num_epochs+1),\n","             train_acc_list, label='Training')\n","    plt.plot(np.arange(1, num_epochs+1),\n","             valid_acc_list, label='Validation')\n","\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","\n","    if results_dir is not None:\n","        image_path = os.path.join(\n","            results_dir, '/kaggle/working/data/plot_acc_training_validation.pdf')\n","        plt.savefig(image_path)"]},{"cell_type":"code","execution_count":12,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2024-08-01T05:54:27.640054Z","iopub.status.busy":"2024-08-01T05:54:27.639784Z","iopub.status.idle":"2024-08-01T05:54:27.652564Z","shell.execute_reply":"2024-08-01T05:54:27.651604Z","shell.execute_reply.started":"2024-08-01T05:54:27.640031Z"},"trusted":true},"outputs":[],"source":["def compute_accuracy(model, data_loader, device):\n","\n","    with torch.no_grad():\n","\n","        correct_pred, num_examples = 0, 0\n","\n","        for i, (features, targets) in enumerate(data_loader):\n","\n","            features = features.to(device)\n","            targets = targets.float().to(device)\n","\n","            logits = model(features)\n","            _, predicted_labels = torch.max(logits, 1)\n","\n","            num_examples += targets.size(0)\n","            correct_pred += (predicted_labels == targets).sum()\n","    return correct_pred.float()/num_examples * 100"]},{"cell_type":"code","execution_count":13,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2024-08-01T05:54:27.654286Z","iopub.status.busy":"2024-08-01T05:54:27.653975Z","iopub.status.idle":"2024-08-01T05:54:27.671082Z","shell.execute_reply":"2024-08-01T05:54:27.670146Z","shell.execute_reply.started":"2024-08-01T05:54:27.654262Z"},"trusted":true},"outputs":[],"source":["def train_model(model, num_epochs, train_loader,\n","                valid_loader, test_loader, optimizer,\n","                device, logging_interval=50,\n","                scheduler=None,\n","                scheduler_on='valid_acc',\n","                start_epoch=0,\n","                checkpoint_path='/kaggle/working/data/checkpoint1.pth.tar'):\n","\n","    start_time = time.time()\n","    minibatch_loss_list, train_acc_list, valid_acc_list = [], [], []\n","    train_loss_list, valid_loss_list = [], []\n","\n","    for epoch in range(start_epoch, num_epochs):\n","\n","        model.train()\n","        running_loss = 0.0\n","        for batch_idx, (features, targets) in enumerate(train_loader):\n","\n","            features = features.to(device)\n","            targets = targets.to(device)\n","\n","            # ## FORWARD AND BACK PROP\n","            logits = model(features)\n","            loss = torch.nn.functional.cross_entropy(logits, targets)\n","            optimizer.zero_grad()\n","            loss.backward()\n","\n","            # ## UPDATE MODEL PARAMETERS\n","            optimizer.step()\n","\n","            # ## LOGGING\n","            minibatch_loss_list.append(loss.item())\n","            running_loss += loss.item() * features.size(0)\n","            if not batch_idx % logging_interval:\n","                print(f'Epoch: {epoch+1:03d}/{num_epochs:03d} '\n","                      f'| Batch {batch_idx:04d}/{len(train_loader):04d} '\n","                      f'| Loss: {loss:.4f}')\n","\n","        train_loss = running_loss / len(train_loader.dataset)\n","        train_loss_list.append(train_loss)\n","\n","        model.eval()\n","        running_val_loss = 0.0\n","        with torch.no_grad():  # save memory during inference\n","            train_acc = compute_accuracy(model, train_loader, device=device)\n","            valid_acc = compute_accuracy(model, valid_loader, device=device)\n","            \n","            for features, targets in valid_loader:\n","                features, targets = features.to(device), targets.to(device)\n","                logits = model(features)\n","                loss = torch.nn.functional.cross_entropy(logits, targets)\n","                running_val_loss += loss.item() * features.size(0)\n","                \n","            valid_loss = running_val_loss / len(valid_loader.dataset)\n","            valid_loss_list.append(valid_loss)\n","\n","            print(f'Epoch: {epoch+1:03d}/{num_epochs:03d} '\n","                  f'| Train Loss: {train_loss:.4f} | Validation Loss: {valid_loss:.4f} '\n","                  f'| Train: {train_acc :.2f}% | Validation: {valid_acc :.2f}%')\n","            train_acc_list.append(train_acc.item())\n","            valid_acc_list.append(valid_acc.item())\n","\n","        elapsed = (time.time() - start_time)/60\n","        print(f'Time elapsed: {elapsed:.2f} min')\n","\n","        if scheduler is not None:\n","            if scheduler_on == 'valid_acc':\n","                scheduler.step(valid_acc_list[-1])\n","            elif scheduler_on == 'minibatch_loss':\n","                scheduler.step(minibatch_loss_list[-1])\n","            else:\n","                raise ValueError(f'Invalid `scheduler_on` choice.')\n","\n","        # Save checkpoint\n","        save_checkpoint({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","        }, filename=checkpoint_path)\n","\n","    elapsed = (time.time() - start_time)/60\n","    print(f'Total Training Time: {elapsed:.2f} min')\n","\n","    test_acc = compute_accuracy(model, test_loader, device=device)\n","    print(f'Test accuracy {test_acc :.2f}%')\n","\n","    return minibatch_loss_list, train_acc_list, valid_acc_list, train_loss_list, valid_loss_list\n"]},{"cell_type":"code","execution_count":26,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2024-08-01T11:42:54.619664Z","iopub.status.busy":"2024-08-01T11:42:54.619302Z","iopub.status.idle":"2024-08-01T11:42:54.625891Z","shell.execute_reply":"2024-08-01T11:42:54.624926Z","shell.execute_reply.started":"2024-08-01T11:42:54.619636Z"},"trusted":true},"outputs":[],"source":["def plot_epoch_losses(train_loss_list, valid_loss_list):\n","    epochs = range(1, len(train_loss_list) + 1)\n","    plt.plot(epochs, train_loss_list, label='Train Loss')\n","    plt.plot(epochs, valid_loss_list, label='Validation Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.title('Train and Validation Loss per Epoch')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.show()\n","    plt.savefig(os.path.join(result_dir, 'plot_accuracy.pdf'))"]},{"cell_type":"code","execution_count":15,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2024-08-01T05:54:27.685731Z","iopub.status.busy":"2024-08-01T05:54:27.685406Z","iopub.status.idle":"2024-08-01T10:24:47.824005Z","shell.execute_reply":"2024-08-01T10:24:47.822342Z","shell.execute_reply.started":"2024-08-01T05:54:27.685707Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["No checkpoint found, starting from scratch\n","Epoch: 001/030 | Batch 0000/0703 | Loss: 5.3999\n","Epoch: 001/030 | Batch 0100/0703 | Loss: 2.6478\n","Epoch: 001/030 | Batch 0200/0703 | Loss: 2.2804\n","Epoch: 001/030 | Batch 0300/0703 | Loss: 2.2120\n","Epoch: 001/030 | Batch 0400/0703 | Loss: 2.2044\n","Epoch: 001/030 | Batch 0500/0703 | Loss: 2.1252\n","Epoch: 001/030 | Batch 0600/0703 | Loss: 2.2128\n","Epoch: 001/030 | Batch 0700/0703 | Loss: 2.1666\n","Epoch: 001/030 | Train Loss: 3.5026 | Validation Loss: 0.2056 | Train: 23.54% | Validation: 24.44%\n","Time elapsed: 8.93 min\n","Epoch: 002/030 | Batch 0000/0703 | Loss: 2.1129\n","Epoch: 002/030 | Batch 0100/0703 | Loss: 2.1739\n","Epoch: 002/030 | Batch 0200/0703 | Loss: 2.2772\n","Epoch: 002/030 | Batch 0300/0703 | Loss: 2.4276\n","Epoch: 002/030 | Batch 0400/0703 | Loss: 1.9112\n","Epoch: 002/030 | Batch 0500/0703 | Loss: 2.0038\n","Epoch: 002/030 | Batch 0600/0703 | Loss: 1.9091\n","Epoch: 002/030 | Batch 0700/0703 | Loss: 1.9954\n","Epoch: 002/030 | Train Loss: 1.8330 | Validation Loss: 0.1879 | Train: 26.92% | Validation: 27.44%\n","Time elapsed: 17.90 min\n","Epoch: 003/030 | Batch 0000/0703 | Loss: 2.0372\n","Epoch: 003/030 | Batch 0100/0703 | Loss: 1.9248\n","Epoch: 003/030 | Batch 0200/0703 | Loss: 1.8756\n","Epoch: 003/030 | Batch 0300/0703 | Loss: 1.9880\n","Epoch: 003/030 | Batch 0400/0703 | Loss: 1.9500\n","Epoch: 003/030 | Batch 0500/0703 | Loss: 2.0148\n","Epoch: 003/030 | Batch 0600/0703 | Loss: 2.0236\n","Epoch: 003/030 | Batch 0700/0703 | Loss: 1.7812\n","Epoch: 003/030 | Train Loss: 1.7412 | Validation Loss: 0.1762 | Train: 34.23% | Validation: 34.80%\n","Time elapsed: 26.90 min\n","Epoch: 004/030 | Batch 0000/0703 | Loss: 1.8940\n","Epoch: 004/030 | Batch 0100/0703 | Loss: 1.7096\n","Epoch: 004/030 | Batch 0200/0703 | Loss: 1.6254\n","Epoch: 004/030 | Batch 0300/0703 | Loss: 1.6599\n","Epoch: 004/030 | Batch 0400/0703 | Loss: 2.0688\n","Epoch: 004/030 | Batch 0500/0703 | Loss: 1.6177\n","Epoch: 004/030 | Batch 0600/0703 | Loss: 1.4531\n","Epoch: 004/030 | Batch 0700/0703 | Loss: 1.6219\n","Epoch: 004/030 | Train Loss: 1.5653 | Validation Loss: 0.1511 | Train: 43.59% | Validation: 44.18%\n","Time elapsed: 35.89 min\n","Epoch: 005/030 | Batch 0000/0703 | Loss: 1.7519\n","Epoch: 005/030 | Batch 0100/0703 | Loss: 1.3191\n","Epoch: 005/030 | Batch 0200/0703 | Loss: 1.4079\n","Epoch: 005/030 | Batch 0300/0703 | Loss: 1.6561\n","Epoch: 005/030 | Batch 0400/0703 | Loss: 1.5753\n","Epoch: 005/030 | Batch 0500/0703 | Loss: 1.5179\n","Epoch: 005/030 | Batch 0600/0703 | Loss: 1.5752\n","Epoch: 005/030 | Batch 0700/0703 | Loss: 1.9010\n","Epoch: 005/030 | Train Loss: 1.4285 | Validation Loss: 0.1403 | Train: 48.45% | Validation: 49.22%\n","Time elapsed: 44.88 min\n","Epoch: 006/030 | Batch 0000/0703 | Loss: 1.4815\n","Epoch: 006/030 | Batch 0100/0703 | Loss: 1.6007\n","Epoch: 006/030 | Batch 0200/0703 | Loss: 1.2785\n","Epoch: 006/030 | Batch 0300/0703 | Loss: 1.3420\n","Epoch: 006/030 | Batch 0400/0703 | Loss: 1.5676\n","Epoch: 006/030 | Batch 0500/0703 | Loss: 1.4385\n","Epoch: 006/030 | Batch 0600/0703 | Loss: 1.4783\n","Epoch: 006/030 | Batch 0700/0703 | Loss: 1.1814\n","Epoch: 006/030 | Train Loss: 1.2580 | Validation Loss: 0.1139 | Train: 60.19% | Validation: 59.96%\n","Time elapsed: 53.87 min\n","Epoch: 007/030 | Batch 0000/0703 | Loss: 1.1625\n","Epoch: 007/030 | Batch 0100/0703 | Loss: 1.3909\n","Epoch: 007/030 | Batch 0200/0703 | Loss: 1.1250\n","Epoch: 007/030 | Batch 0300/0703 | Loss: 1.2972\n","Epoch: 007/030 | Batch 0400/0703 | Loss: 1.2581\n","Epoch: 007/030 | Batch 0500/0703 | Loss: 1.1434\n","Epoch: 007/030 | Batch 0600/0703 | Loss: 1.2765\n","Epoch: 007/030 | Batch 0700/0703 | Loss: 1.3213\n","Epoch: 007/030 | Train Loss: 1.0515 | Validation Loss: 0.1001 | Train: 65.65% | Validation: 64.86%\n","Time elapsed: 62.87 min\n","Epoch: 008/030 | Batch 0000/0703 | Loss: 1.0229\n","Epoch: 008/030 | Batch 0100/0703 | Loss: 1.1705\n","Epoch: 008/030 | Batch 0200/0703 | Loss: 1.1416\n","Epoch: 008/030 | Batch 0300/0703 | Loss: 1.1753\n","Epoch: 008/030 | Batch 0400/0703 | Loss: 1.2837\n","Epoch: 008/030 | Batch 0500/0703 | Loss: 0.8952\n","Epoch: 008/030 | Batch 0600/0703 | Loss: 1.0593\n","Epoch: 008/030 | Batch 0700/0703 | Loss: 0.9625\n","Epoch: 008/030 | Train Loss: 1.0324 | Validation Loss: 0.0970 | Train: 67.15% | Validation: 65.96%\n","Time elapsed: 71.87 min\n","Epoch: 009/030 | Batch 0000/0703 | Loss: 1.0271\n","Epoch: 009/030 | Batch 0100/0703 | Loss: 0.9598\n","Epoch: 009/030 | Batch 0200/0703 | Loss: 0.9567\n","Epoch: 009/030 | Batch 0300/0703 | Loss: 2.3288\n","Epoch: 009/030 | Batch 0400/0703 | Loss: 2.2998\n","Epoch: 009/030 | Batch 0500/0703 | Loss: 1.1067\n","Epoch: 009/030 | Batch 0600/0703 | Loss: 0.7390\n","Epoch: 009/030 | Batch 0700/0703 | Loss: 0.9929\n","Epoch: 009/030 | Train Loss: 1.3691 | Validation Loss: 0.0978 | Train: 66.73% | Validation: 65.12%\n","Time elapsed: 80.86 min\n","Epoch: 010/030 | Batch 0000/0703 | Loss: 0.9480\n","Epoch: 010/030 | Batch 0100/0703 | Loss: 0.9192\n","Epoch: 010/030 | Batch 0200/0703 | Loss: 1.2264\n","Epoch: 010/030 | Batch 0300/0703 | Loss: 1.0704\n","Epoch: 010/030 | Batch 0400/0703 | Loss: 0.8265\n","Epoch: 010/030 | Batch 0500/0703 | Loss: 0.9706\n","Epoch: 010/030 | Batch 0600/0703 | Loss: 0.8455\n","Epoch: 010/030 | Batch 0700/0703 | Loss: 0.7658\n","Epoch: 010/030 | Train Loss: 0.8505 | Validation Loss: 0.0764 | Train: 76.03% | Validation: 73.32%\n","Time elapsed: 89.86 min\n","Epoch: 011/030 | Batch 0000/0703 | Loss: 0.9390\n","Epoch: 011/030 | Batch 0100/0703 | Loss: 0.8820\n","Epoch: 011/030 | Batch 0200/0703 | Loss: 1.0197\n","Epoch: 011/030 | Batch 0300/0703 | Loss: 0.8641\n","Epoch: 011/030 | Batch 0400/0703 | Loss: 0.8649\n","Epoch: 011/030 | Batch 0500/0703 | Loss: 0.9020\n","Epoch: 011/030 | Batch 0600/0703 | Loss: 0.7702\n","Epoch: 011/030 | Batch 0700/0703 | Loss: 0.7656\n","Epoch: 011/030 | Train Loss: 0.7297 | Validation Loss: 0.0702 | Train: 79.50% | Validation: 76.22%\n","Time elapsed: 98.85 min\n","Epoch: 012/030 | Batch 0000/0703 | Loss: 0.8504\n","Epoch: 012/030 | Batch 0100/0703 | Loss: 0.7663\n","Epoch: 012/030 | Batch 0200/0703 | Loss: 0.6458\n","Epoch: 012/030 | Batch 0300/0703 | Loss: 0.8809\n","Epoch: 012/030 | Batch 0400/0703 | Loss: 0.8600\n","Epoch: 012/030 | Batch 0500/0703 | Loss: 0.7944\n","Epoch: 012/030 | Batch 0600/0703 | Loss: 0.6257\n","Epoch: 012/030 | Batch 0700/0703 | Loss: 0.5144\n","Epoch: 012/030 | Train Loss: 0.6789 | Validation Loss: 0.0643 | Train: 83.16% | Validation: 77.86%\n","Time elapsed: 107.84 min\n","Epoch: 013/030 | Batch 0000/0703 | Loss: 0.7743\n","Epoch: 013/030 | Batch 0100/0703 | Loss: 0.3870\n","Epoch: 013/030 | Batch 0200/0703 | Loss: 0.6515\n","Epoch: 013/030 | Batch 0300/0703 | Loss: 0.5992\n","Epoch: 013/030 | Batch 0400/0703 | Loss: 0.6055\n","Epoch: 013/030 | Batch 0500/0703 | Loss: 0.7145\n","Epoch: 013/030 | Batch 0600/0703 | Loss: 0.4650\n","Epoch: 013/030 | Batch 0700/0703 | Loss: 0.7831\n","Epoch: 013/030 | Train Loss: 0.6094 | Validation Loss: 0.0684 | Train: 82.72% | Validation: 76.60%\n","Time elapsed: 116.84 min\n","Epoch: 014/030 | Batch 0000/0703 | Loss: 0.6913\n","Epoch: 014/030 | Batch 0100/0703 | Loss: 0.7982\n","Epoch: 014/030 | Batch 0200/0703 | Loss: 0.5574\n","Epoch: 014/030 | Batch 0300/0703 | Loss: 0.5666\n","Epoch: 014/030 | Batch 0400/0703 | Loss: 0.6912\n","Epoch: 014/030 | Batch 0500/0703 | Loss: 0.6261\n","Epoch: 014/030 | Batch 0600/0703 | Loss: 0.5229\n","Epoch: 014/030 | Batch 0700/0703 | Loss: 0.7367\n","Epoch: 014/030 | Train Loss: 0.5276 | Validation Loss: 0.0632 | Train: 85.36% | Validation: 78.28%\n","Time elapsed: 125.83 min\n","Epoch: 015/030 | Batch 0000/0703 | Loss: 0.3333\n","Epoch: 015/030 | Batch 0100/0703 | Loss: 0.7420\n","Epoch: 015/030 | Batch 0200/0703 | Loss: 0.4729\n","Epoch: 015/030 | Batch 0300/0703 | Loss: 0.3272\n","Epoch: 015/030 | Batch 0400/0703 | Loss: 0.4199\n","Epoch: 015/030 | Batch 0500/0703 | Loss: 0.4242\n","Epoch: 015/030 | Batch 0600/0703 | Loss: 0.5318\n","Epoch: 015/030 | Batch 0700/0703 | Loss: 0.6625\n","Epoch: 015/030 | Train Loss: 0.4778 | Validation Loss: 0.0590 | Train: 88.51% | Validation: 80.22%\n","Time elapsed: 134.83 min\n","Epoch: 016/030 | Batch 0000/0703 | Loss: 0.5741\n","Epoch: 016/030 | Batch 0100/0703 | Loss: 0.6425\n","Epoch: 016/030 | Batch 0200/0703 | Loss: 0.4997\n","Epoch: 016/030 | Batch 0300/0703 | Loss: 0.5335\n","Epoch: 016/030 | Batch 0400/0703 | Loss: 0.4355\n","Epoch: 016/030 | Batch 0500/0703 | Loss: 0.4876\n","Epoch: 016/030 | Batch 0600/0703 | Loss: 0.5470\n","Epoch: 016/030 | Batch 0700/0703 | Loss: 0.3037\n","Epoch: 016/030 | Train Loss: 0.4694 | Validation Loss: 0.0588 | Train: 90.49% | Validation: 80.62%\n","Time elapsed: 143.82 min\n","Epoch: 017/030 | Batch 0000/0703 | Loss: 0.3449\n","Epoch: 017/030 | Batch 0100/0703 | Loss: 0.3141\n","Epoch: 017/030 | Batch 0200/0703 | Loss: 0.3438\n","Epoch: 017/030 | Batch 0300/0703 | Loss: 0.8508\n","Epoch: 017/030 | Batch 0400/0703 | Loss: 0.6125\n","Epoch: 017/030 | Batch 0500/0703 | Loss: 0.5867\n","Epoch: 017/030 | Batch 0600/0703 | Loss: 0.4948\n","Epoch: 017/030 | Batch 0700/0703 | Loss: 0.3586\n","Epoch: 017/030 | Train Loss: 0.4855 | Validation Loss: 0.0613 | Train: 90.54% | Validation: 79.98%\n","Time elapsed: 152.82 min\n","Epoch: 018/030 | Batch 0000/0703 | Loss: 0.2700\n","Epoch: 018/030 | Batch 0100/0703 | Loss: 0.1570\n","Epoch: 018/030 | Batch 0200/0703 | Loss: 0.3771\n","Epoch: 018/030 | Batch 0300/0703 | Loss: 0.2944\n","Epoch: 018/030 | Batch 0400/0703 | Loss: 0.3154\n","Epoch: 018/030 | Batch 0500/0703 | Loss: 0.5478\n","Epoch: 018/030 | Batch 0600/0703 | Loss: 0.3330\n","Epoch: 018/030 | Batch 0700/0703 | Loss: 0.4334\n","Epoch: 018/030 | Train Loss: 0.3646 | Validation Loss: 0.0543 | Train: 94.03% | Validation: 82.26%\n","Time elapsed: 161.83 min\n","Epoch: 019/030 | Batch 0000/0703 | Loss: 0.4108\n","Epoch: 019/030 | Batch 0100/0703 | Loss: 0.1700\n","Epoch: 019/030 | Batch 0200/0703 | Loss: 0.5167\n","Epoch: 019/030 | Batch 0300/0703 | Loss: 0.6328\n","Epoch: 019/030 | Batch 0400/0703 | Loss: 0.2730\n","Epoch: 019/030 | Batch 0500/0703 | Loss: 0.4839\n","Epoch: 019/030 | Batch 0600/0703 | Loss: 0.2051\n","Epoch: 019/030 | Batch 0700/0703 | Loss: 0.1653\n","Epoch: 019/030 | Train Loss: 0.3119 | Validation Loss: 0.0586 | Train: 93.94% | Validation: 81.10%\n","Time elapsed: 170.83 min\n","Epoch: 020/030 | Batch 0000/0703 | Loss: 0.3102\n","Epoch: 020/030 | Batch 0100/0703 | Loss: 0.3047\n","Epoch: 020/030 | Batch 0200/0703 | Loss: 0.3679\n","Epoch: 020/030 | Batch 0300/0703 | Loss: 0.2418\n","Epoch: 020/030 | Batch 0400/0703 | Loss: 0.2925\n","Epoch: 020/030 | Batch 0500/0703 | Loss: 0.3437\n","Epoch: 020/030 | Batch 0600/0703 | Loss: 0.4235\n","Epoch: 020/030 | Batch 0700/0703 | Loss: 0.3413\n","Epoch: 020/030 | Train Loss: 0.2938 | Validation Loss: 0.0614 | Train: 94.90% | Validation: 81.50%\n","Time elapsed: 179.83 min\n","Epoch: 021/030 | Batch 0000/0703 | Loss: 0.2943\n","Epoch: 021/030 | Batch 0100/0703 | Loss: 0.3769\n","Epoch: 021/030 | Batch 0200/0703 | Loss: 0.2207\n","Epoch: 021/030 | Batch 0300/0703 | Loss: 0.2629\n","Epoch: 021/030 | Batch 0400/0703 | Loss: 0.3290\n","Epoch: 021/030 | Batch 0500/0703 | Loss: 0.3942\n","Epoch: 021/030 | Batch 0600/0703 | Loss: 0.2641\n","Epoch: 021/030 | Batch 0700/0703 | Loss: 0.3240\n","Epoch: 021/030 | Train Loss: 0.2934 | Validation Loss: 0.0616 | Train: 95.19% | Validation: 81.74%\n","Time elapsed: 188.82 min\n","Epoch: 022/030 | Batch 0000/0703 | Loss: 0.2692\n","Epoch: 022/030 | Batch 0100/0703 | Loss: 0.4852\n","Epoch: 022/030 | Batch 0200/0703 | Loss: 0.2366\n","Epoch: 022/030 | Batch 0300/0703 | Loss: 0.2821\n","Epoch: 022/030 | Batch 0400/0703 | Loss: 0.3246\n","Epoch: 022/030 | Batch 0500/0703 | Loss: 0.2923\n","Epoch: 022/030 | Batch 0600/0703 | Loss: 0.2460\n","Epoch: 022/030 | Batch 0700/0703 | Loss: 0.1969\n","Epoch: 022/030 | Train Loss: 0.3129 | Validation Loss: 0.0578 | Train: 97.04% | Validation: 82.76%\n","Time elapsed: 197.82 min\n","Epoch: 023/030 | Batch 0000/0703 | Loss: 0.3337\n","Epoch: 023/030 | Batch 0100/0703 | Loss: 0.1643\n","Epoch: 023/030 | Batch 0200/0703 | Loss: 0.3716\n","Epoch: 023/030 | Batch 0300/0703 | Loss: 0.2053\n","Epoch: 023/030 | Batch 0400/0703 | Loss: 0.1346\n","Epoch: 023/030 | Batch 0500/0703 | Loss: 0.0993\n","Epoch: 023/030 | Batch 0600/0703 | Loss: 0.1762\n","Epoch: 023/030 | Batch 0700/0703 | Loss: 0.2163\n","Epoch: 023/030 | Train Loss: 0.2382 | Validation Loss: 0.0599 | Train: 97.65% | Validation: 82.94%\n","Time elapsed: 206.82 min\n","Epoch: 024/030 | Batch 0000/0703 | Loss: 0.1426\n","Epoch: 024/030 | Batch 0100/0703 | Loss: 0.2494\n","Epoch: 024/030 | Batch 0200/0703 | Loss: 0.1922\n","Epoch: 024/030 | Batch 0300/0703 | Loss: 0.2177\n","Epoch: 024/030 | Batch 0400/0703 | Loss: 0.1919\n","Epoch: 024/030 | Batch 0500/0703 | Loss: 0.3401\n","Epoch: 024/030 | Batch 0600/0703 | Loss: 0.1783\n","Epoch: 024/030 | Batch 0700/0703 | Loss: 0.2086\n","Epoch: 024/030 | Train Loss: 0.2331 | Validation Loss: 0.0636 | Train: 96.85% | Validation: 82.46%\n","Time elapsed: 215.82 min\n","Epoch: 025/030 | Batch 0000/0703 | Loss: 0.1526\n","Epoch: 025/030 | Batch 0100/0703 | Loss: 0.2620\n","Epoch: 025/030 | Batch 0200/0703 | Loss: 0.2308\n","Epoch: 025/030 | Batch 0300/0703 | Loss: 0.1450\n","Epoch: 025/030 | Batch 0400/0703 | Loss: 0.3720\n","Epoch: 025/030 | Batch 0500/0703 | Loss: 0.2690\n","Epoch: 025/030 | Batch 0600/0703 | Loss: 0.4220\n","Epoch: 025/030 | Batch 0700/0703 | Loss: 0.2229\n","Epoch: 025/030 | Train Loss: 0.3290 | Validation Loss: 0.0710 | Train: 94.18% | Validation: 79.42%\n","Time elapsed: 224.81 min\n","Epoch: 026/030 | Batch 0000/0703 | Loss: 0.3273\n","Epoch: 026/030 | Batch 0100/0703 | Loss: 0.2358\n","Epoch: 026/030 | Batch 0200/0703 | Loss: 0.4082\n","Epoch: 026/030 | Batch 0300/0703 | Loss: 0.2741\n","Epoch: 026/030 | Batch 0400/0703 | Loss: 0.2066\n","Epoch: 026/030 | Batch 0500/0703 | Loss: 0.2763\n","Epoch: 026/030 | Batch 0600/0703 | Loss: 0.1084\n","Epoch: 026/030 | Batch 0700/0703 | Loss: 0.2098\n","Epoch: 026/030 | Train Loss: 0.2479 | Validation Loss: 0.0634 | Train: 97.54% | Validation: 81.70%\n","Time elapsed: 233.81 min\n","Epoch: 027/030 | Batch 0000/0703 | Loss: 0.1250\n","Epoch: 027/030 | Batch 0100/0703 | Loss: 0.0963\n","Epoch: 027/030 | Batch 0200/0703 | Loss: 0.3748\n","Epoch: 027/030 | Batch 0300/0703 | Loss: 0.3095\n","Epoch: 027/030 | Batch 0400/0703 | Loss: 4.1293\n","Epoch: 027/030 | Batch 0500/0703 | Loss: 0.2451\n","Epoch: 027/030 | Batch 0600/0703 | Loss: 0.3625\n","Epoch: 027/030 | Batch 0700/0703 | Loss: 0.2304\n","Epoch: 027/030 | Train Loss: 0.3072 | Validation Loss: 0.0661 | Train: 94.84% | Validation: 80.38%\n","Time elapsed: 242.80 min\n","Epoch: 028/030 | Batch 0000/0703 | Loss: 0.2382\n","Epoch: 028/030 | Batch 0100/0703 | Loss: 0.2217\n","Epoch: 028/030 | Batch 0200/0703 | Loss: 0.2567\n","Epoch: 028/030 | Batch 0300/0703 | Loss: 0.1779\n","Epoch: 028/030 | Batch 0400/0703 | Loss: 0.2109\n","Epoch: 028/030 | Batch 0500/0703 | Loss: 0.2581\n","Epoch: 028/030 | Batch 0600/0703 | Loss: 0.1148\n","Epoch: 028/030 | Batch 0700/0703 | Loss: 0.1785\n","Epoch: 028/030 | Train Loss: 0.2228 | Validation Loss: 0.0604 | Train: 97.80% | Validation: 83.12%\n","Time elapsed: 251.79 min\n","Epoch: 029/030 | Batch 0000/0703 | Loss: 0.3101\n","Epoch: 029/030 | Batch 0100/0703 | Loss: 1.8163\n","Epoch: 029/030 | Batch 0200/0703 | Loss: 0.1388\n","Epoch: 029/030 | Batch 0300/0703 | Loss: 0.6542\n","Epoch: 029/030 | Batch 0400/0703 | Loss: 0.1691\n","Epoch: 029/030 | Batch 0500/0703 | Loss: 0.1300\n","Epoch: 029/030 | Batch 0600/0703 | Loss: 0.2579\n","Epoch: 029/030 | Batch 0700/0703 | Loss: 0.1936\n","Epoch: 029/030 | Train Loss: 0.2111 | Validation Loss: 0.0674 | Train: 95.70% | Validation: 81.68%\n","Time elapsed: 260.79 min\n","Epoch: 030/030 | Batch 0000/0703 | Loss: 0.1997\n","Epoch: 030/030 | Batch 0100/0703 | Loss: 0.1014\n","Epoch: 030/030 | Batch 0200/0703 | Loss: 0.2092\n","Epoch: 030/030 | Batch 0300/0703 | Loss: 0.2506\n","Epoch: 030/030 | Batch 0400/0703 | Loss: 0.4285\n","Epoch: 030/030 | Batch 0500/0703 | Loss: 0.3640\n","Epoch: 030/030 | Batch 0600/0703 | Loss: 0.3333\n","Epoch: 030/030 | Batch 0700/0703 | Loss: 0.2532\n","Epoch: 030/030 | Train Loss: 0.2878 | Validation Loss: 0.0777 | Train: 93.23% | Validation: 78.70%\n","Time elapsed: 269.78 min\n","Total Training Time: 269.84 min\n","Test accuracy 77.39%\n"]},{"ename":"ValueError","evalue":"too many values to unpack (expected 3)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[15], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo checkpoint found, starting from scratch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# 학습 및 평가 루프\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m minibatch_loss_list, train_acc_list, valid_acc_list \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[1;32m     37\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     38\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m     39\u001b[0m     scheduler\u001b[38;5;241m=\u001b[39mscheduler,\n\u001b[1;32m     40\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs,\n\u001b[1;32m     41\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[1;32m     42\u001b[0m     valid_loader\u001b[38;5;241m=\u001b[39mvalid_loader,\n\u001b[1;32m     43\u001b[0m     test_loader\u001b[38;5;241m=\u001b[39mtest_loader,\n\u001b[1;32m     44\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     45\u001b[0m     scheduler_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid_acc\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     46\u001b[0m     start_epoch\u001b[38;5;241m=\u001b[39mstart_epoch,\n\u001b[1;32m     47\u001b[0m     checkpoint_path\u001b[38;5;241m=\u001b[39mcheckpoint_path,\n\u001b[1;32m     48\u001b[0m     logging_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     50\u001b[0m plot_training_loss(minibatch_loss_list\u001b[38;5;241m=\u001b[39mminibatch_loss_list,\n\u001b[1;32m     51\u001b[0m                    num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs,\n\u001b[1;32m     52\u001b[0m                    iter_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader),\n\u001b[1;32m     53\u001b[0m                    results_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     54\u001b[0m                    averaging_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[1;32m     55\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"]}],"source":["# hyper parameters\n","num_classes = 10\n","num_epochs = 30\n","batch_size = 64\n","learning_rate = 0.001\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = VGG16(num_classes).to(device)\n","\n","# loss and optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n","                                                       factor=0.1,\n","                                                       mode='max',\n","                                                       verbose=True)\n","# checkpoints\n","def save_checkpoint(state, filename='/kaggle/working/data/checkpoint1.pth.tar'):\n","    torch.save(state, filename)\n","\n","# load checkpoints\n","def load_checkpoint(filename):\n","    checkpoint = torch.load(filename)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    epoch = checkpoint['epoch']\n","    return epoch\n","    \n","start_epoch = 0\n","checkpoint_path = '/kaggle/working/data/checkpoint1.pth.tar'\n","try:\n","    start_epoch = load_checkpoint(checkpoint_path)\n","    print(f'Checkpoint loaded, starting from epoch {start_epoch + 1}')\n","except FileNotFoundError:\n","    print('No checkpoint found, starting from scratch')\n","\n","# train and evaluation\n","minibatch_loss_list, train_acc_list, valid_acc_list = train_model(\n","    model=model,\n","    optimizer=optimizer,\n","    scheduler=scheduler,\n","    num_epochs=num_epochs,\n","    train_loader=train_loader,\n","    valid_loader=valid_loader,\n","    test_loader=test_loader,\n","    device=device,\n","    scheduler_on='valid_acc',\n","    start_epoch=start_epoch,\n","    checkpoint_path=checkpoint_path,\n","    logging_interval=100\n",")\n","plot_training_loss(minibatch_loss_list=minibatch_loss_list,\n","                   num_epochs=num_epochs,\n","                   iter_per_epoch=len(train_loader),\n","                   results_dir='None',\n","                   averaging_iterations=200)\n","plt.show()\n","\n","plot_accuracy(train_acc_list=train_acc_list, \n","              valid_acc_list=valid_acc_list,\n","              result_dri='None')\n","plt.ylim([60, 100])\n","plt.show()\n","\n","plot_epoch_losses(train_loss_list, valid_loss_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.status.busy":"2024-08-01T10:24:47.825383Z","iopub.status.idle":"2024-08-01T10:24:47.825872Z","shell.execute_reply":"2024-08-01T10:24:47.825645Z","shell.execute_reply.started":"2024-08-01T10:24:47.825626Z"},"trusted":true},"outputs":[],"source":["# # testing\n","# with torch.no_grad():\n","#     correct = 0\n","#     total = 0\n","#     for images, labels in test_loader:\n","#         images = images.to(device)\n","#         labels = labels.to(device)\n","#         outputs = model(images)\n","\n","#         _, predicted = torch.max(outputs.data, 1)\n","#         total += labels.size(0)\n","#         correct += (predicted == labels).sum().item()\n","#         del images, labels, outputs\n","\n","#     accuracy = (correct / total) * 100\n","#     print('Accuracy of the network on the {} test images: {:.6f} %'.format(total, accuracy))"]},{"cell_type":"code","execution_count":null,"metadata":{"editable":false,"execution":{"iopub.execute_input":"2024-08-01T11:56:11.265272Z","iopub.status.busy":"2024-08-01T11:56:11.264894Z"},"trusted":true},"outputs":[],"source":["def save_checkpoint(model, optimizer, epoch, path='/kaggle/working/data/checkpoint.pth'):\n","    state = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","    }\n","    torch.save(state, path)\n","    print(f'Model saved to {path}')\n","\n","# save final model\n","save_checkpoint(model, optimizer, num_epochs)\n","\n","# 런타임 해제 코드\n","import os\n","os.system('kill -9 -1')"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
