{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNOx3Ex3KN2B",
        "outputId": "49a9d00e-4a1a-4502-f20c-89ae0194570e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Vision_Transformer'...\n",
            "remote: Enumerating objects: 440, done.\u001b[K\n",
            "remote: Counting objects: 100% (129/129), done.\u001b[K\n",
            "remote: Compressing objects: 100% (81/81), done.\u001b[K\n",
            "remote: Total 440 (delta 95), reused 73 (delta 48), pack-reused 311 (from 1)\u001b[K\n",
            "Receiving objects: 100% (440/440), 1.95 MiB | 11.69 MiB/s, done.\n",
            "Resolving deltas: 100% (262/262), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/PARKYUNSU/Vision_Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yiMADZyKRLm",
        "outputId": "992e6c44-137b-45be-a5b9-d248eb44d0c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-03 04:33:15--  https://huggingface.co/google/vit-base-patch16-224-in21k/resolve/main/pytorch_model.bin\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.4, 18.172.134.88, 18.172.134.24, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/google/vit-base-patch16-224-in21k/84066da0f5d8ff1cc494c660d4693141fae2e356535bf18a14d9fc00a055a6a1?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1738560795&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODU2MDc5NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9nb29nbGUvdml0LWJhc2UtcGF0Y2gxNi0yMjQtaW4yMWsvODQwNjZkYTBmNWQ4ZmYxY2M0OTRjNjYwZDQ2OTMxNDFmYWUyZTM1NjUzNWJmMThhMTRkOWZjMDBhMDU1YTZhMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=SHF9A%7EnbLWduE5wIWz9TjlIqJ832yp6B6RuAH0G3QWtSin8vSy-tX1B7mxwkFM7tbWCm4WTcKvwFuMJccrD2HxHc3iEBphs4UxLlqOO1q%7EAOwA4QsgsUMrJeHB3n9rKbD8baBsavZDUdKS8cMRSnCbO4kbIuG7wLFx1YjU4B3FcH6mYsIqzXOzPRlgSEONRRVQnY22xT6DPT9qPjug9kDquhCWa3SQ1LbhrQOeozSvt0PHxxUYBm8BSQ6fOsWZ6NVFktVQ0VNQL%7Ej9n0YzcTKaRXF9-59%7E%7E8lZ2hgGJyUvcl4kD30NKvxPMDAtdGYBHaRy3hTAzJW7GipRroonr4Vw__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2025-02-03 04:33:15--  https://cdn-lfs.hf.co/google/vit-base-patch16-224-in21k/84066da0f5d8ff1cc494c660d4693141fae2e356535bf18a14d9fc00a055a6a1?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1738560795&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczODU2MDc5NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9nb29nbGUvdml0LWJhc2UtcGF0Y2gxNi0yMjQtaW4yMWsvODQwNjZkYTBmNWQ4ZmYxY2M0OTRjNjYwZDQ2OTMxNDFmYWUyZTM1NjUzNWJmMThhMTRkOWZjMDBhMDU1YTZhMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=SHF9A%7EnbLWduE5wIWz9TjlIqJ832yp6B6RuAH0G3QWtSin8vSy-tX1B7mxwkFM7tbWCm4WTcKvwFuMJccrD2HxHc3iEBphs4UxLlqOO1q%7EAOwA4QsgsUMrJeHB3n9rKbD8baBsavZDUdKS8cMRSnCbO4kbIuG7wLFx1YjU4B3FcH6mYsIqzXOzPRlgSEONRRVQnY22xT6DPT9qPjug9kDquhCWa3SQ1LbhrQOeozSvt0PHxxUYBm8BSQ6fOsWZ6NVFktVQ0VNQL%7Ej9n0YzcTKaRXF9-59%7E%7E8lZ2hgGJyUvcl4kD30NKvxPMDAtdGYBHaRy3hTAzJW7GipRroonr4Vw__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 3.167.152.12, 3.167.152.37, 3.167.152.119, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|3.167.152.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 345636463 (330M) [application/octet-stream]\n",
            "Saving to: ‘/content/sample_data/vit_base_patch16_224_in21k.pth’\n",
            "\n",
            "/content/sample_dat 100%[===================>] 329.62M   222MB/s    in 1.5s    \n",
            "\n",
            "2025-02-03 04:33:16 (222 MB/s) - ‘/content/sample_data/vit_base_patch16_224_in21k.pth’ saved [345636463/345636463]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/google/vit-base-patch16-224-in21k/resolve/main/pytorch_model.bin -O /content/sample_data/vit_base_patch16_224_in21k.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhceJj-wKSSl",
        "outputId": "ba2fb61c-255a-46dd-ea48-7ed7228e013e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scipy) (1.26.4)\n",
            "Collecting ml-collections\n",
            "  Downloading ml_collections-1.0.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from ml-collections) (1.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from ml-collections) (1.17.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from ml-collections) (6.0.2)\n",
            "Downloading ml_collections-1.0.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ml-collections\n",
            "Successfully installed ml-collections-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install scipy\n",
        "!pip install ml-collections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbMCbKRJKUN9",
        "outputId": "393c0f5e-9ef3-461c-d2c2-ac63943ccdfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
            "100% 170M/170M [00:02<00:00, 82.3MB/s]\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "/content/Vision_Transformer/main.py:98: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pretrained_weights = torch.load(args.pretrained_path, map_location=device)\n",
            "Warning: 'head/kernel' not found, skipping head weights load.\n",
            "Warning: 'head/bias' not found, skipping head bias load.\n",
            "Loaded weights with message: _IncompatibleKeys(missing_keys=['pos_embed', 'encoder.norm.weight', 'encoder.norm.bias', 'head.weight', 'head.bias'], unexpected_keys=['layernorm.weight', 'layernorm.bias'])\n",
            "Starting training...\n",
            "Training Epoch 1: 100%|███████████████████████████████████████████| 782/782 [06:01<00:00,  2.16it/s]\n",
            "Evaluating: 100%|█████████████████████████████████████████████████| 157/157 [00:23<00:00,  6.76it/s]\n",
            "Epoch [1/10] | Train Loss: 0.1761, Train Acc: 94.52% | Eval Loss: 0.0793, Eval Acc: 97.53%\n",
            "Training Epoch 2: 100%|███████████████████████████████████████████| 782/782 [05:59<00:00,  2.17it/s]\n",
            "Evaluating: 100%|█████████████████████████████████████████████████| 157/157 [00:23<00:00,  6.77it/s]\n",
            "Epoch [2/10] | Train Loss: 0.0818, Train Acc: 97.41% | Eval Loss: 0.0904, Eval Acc: 97.21%\n",
            "Training Epoch 3: 100%|███████████████████████████████████████████| 782/782 [05:59<00:00,  2.17it/s]\n",
            "Evaluating: 100%|█████████████████████████████████████████████████| 157/157 [00:23<00:00,  6.76it/s]\n",
            "Epoch [3/10] | Train Loss: 0.0668, Train Acc: 97.72% | Eval Loss: 0.0770, Eval Acc: 97.72%\n",
            "Training Epoch 4: 100%|███████████████████████████████████████████| 782/782 [05:59<00:00,  2.17it/s]\n",
            "Evaluating: 100%|█████████████████████████████████████████████████| 157/157 [00:23<00:00,  6.77it/s]\n",
            "Epoch [4/10] | Train Loss: 0.0589, Train Acc: 98.06% | Eval Loss: 0.0851, Eval Acc: 97.77%\n",
            "Training Epoch 5: 100%|███████████████████████████████████████████| 782/782 [05:59<00:00,  2.17it/s]\n",
            "Evaluating: 100%|█████████████████████████████████████████████████| 157/157 [00:23<00:00,  6.77it/s]\n",
            "Epoch [5/10] | Train Loss: 0.0440, Train Acc: 98.54% | Eval Loss: 0.0837, Eval Acc: 97.70%\n",
            "Training Epoch 6: 100%|███████████████████████████████████████████| 782/782 [05:59<00:00,  2.17it/s]\n",
            "Evaluating: 100%|█████████████████████████████████████████████████| 157/157 [00:23<00:00,  6.76it/s]\n",
            "Epoch [6/10] | Train Loss: 0.0408, Train Acc: 98.66% | Eval Loss: 0.0732, Eval Acc: 97.95%\n",
            "Training Epoch 7: 100%|███████████████████████████████████████████| 782/782 [05:59<00:00,  2.17it/s]\n",
            "Evaluating: 100%|█████████████████████████████████████████████████| 157/157 [00:23<00:00,  6.77it/s]\n",
            "Epoch [7/10] | Train Loss: 0.0362, Train Acc: 98.79% | Eval Loss: 0.0774, Eval Acc: 98.02%\n",
            "Training Epoch 8: 100%|███████████████████████████████████████████| 782/782 [05:59<00:00,  2.17it/s]\n",
            "Evaluating: 100%|█████████████████████████████████████████████████| 157/157 [00:23<00:00,  6.77it/s]\n",
            "Epoch [8/10] | Train Loss: 0.0329, Train Acc: 98.90% | Eval Loss: 0.0866, Eval Acc: 97.87%\n",
            "Training Epoch 9: 100%|███████████████████████████████████████████| 782/782 [05:59<00:00,  2.17it/s]\n",
            "Evaluating: 100%|█████████████████████████████████████████████████| 157/157 [00:23<00:00,  6.77it/s]\n",
            "Epoch [9/10] | Train Loss: 0.0284, Train Acc: 99.02% | Eval Loss: 0.1116, Eval Acc: 97.29%\n",
            "Training Epoch 10: 100%|██████████████████████████████████████████| 782/782 [05:59<00:00,  2.17it/s]\n",
            "Evaluating: 100%|█████████████████████████████████████████████████| 157/157 [00:23<00:00,  6.76it/s]\n",
            "Epoch [10/10] | Train Loss: 0.0286, Train Acc: 99.03% | Eval Loss: 0.1023, Eval Acc: 97.38%\n",
            "Training finished.\n",
            "Graph saved as 'loss_accuracy_plot.png'\n",
            "Model saved to fine_tuned_model.pth\n"
          ]
        }
      ],
      "source": [
        "!python /content/Vision_Transformer/main.py --mode train --pretrained_path \"/content/sample_data/vit_base_patch16_224_in21k.pth\" --epochs 10 --batch_size 64 --learning_rate 1e-4 --save_fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lctt-ndJOIVa",
        "outputId": "83eb0b44-c776-4157-8467-a21ee8f7f762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting visualization...\n",
            "/content/Vision_Transformer/model/vit.py:33: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.load_from(torch.load(pretrained_path, map_location=\"cpu\"))\n",
            "Warning: 'head/kernel' not found, skipping head weights load.\n",
            "Warning: 'head/bias' not found, skipping head bias load.\n",
            "Loaded weights with message: _IncompatibleKeys(missing_keys=['pos_embed', 'encoder.norm.weight', 'encoder.norm.bias', 'head.weight', 'head.bias'], unexpected_keys=['layernorm.weight', 'layernorm.bias'])\n",
            "Figure saved as attention_map.png\n"
          ]
        }
      ],
      "source": [
        "!python /content/Vision_Transformer/main.py --mode visualize --pretrained_path \"/content/sample_data/vit_base_patch16_224_in21k.pth\" --image_path \"/content/data/my_image.jpg\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}