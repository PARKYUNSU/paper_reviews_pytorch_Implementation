# SimSiam

"Exploring Simple Siamese Representation Learning" - 2020

-Xinlei Chen, Kaiming He (Facebook AI Research)

---

## 1. Introduction

### Siamese Networks

<details> <summary>Siamese Networks ë³´ê¸°</summary>
 
 DeepLearningì—ì„œëŠ” í•™ìŠµì„ ìœ„í•´ ë§ì€ ì–‘ì˜ ë°ì´í„°ë¥¼ í•„ìš”ë¡œ í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë‹¤ëŠ” ë§ì€, DeepLearning ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•ŠìŒì„ ì•”ì‹œí•©ë‹ˆë‹¤.

 ê·¸ë˜ì„œ ê³ ì•ˆëœ Siamese NetworksëŠ” ë°ì´í„° ì–‘ì´ ì ê±°ë‚˜, Imbalanced Class Distributioní•œ ë°ì´í„°ì—ì„œë„ ëª¨ë¸ì˜ ì •í™•ì„±ì„ ë†’í ìˆ˜ ìˆìŠµë‹ˆë‹¤.
 
 Siamese NetworksëŠ” ë™ì¼í•œ parametersë‚˜ weightsì„ ê³µìœ í•˜ëŠ” Twin Networksë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.
 
 ì´ ë„¤íŠ¸ì›Œí¬ëŠ” í•œ ìŒì˜ inputsë¥¼ ë°›ì•„ ê°ê°ì˜ featuresë¥¼ ì¶”ì¶œí•œ ë’¤ ë‘ inputs ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Classificationì„ ìˆ˜í–‰í•˜ë©°, ê°™ì€ Classì˜ ë°ì´í„°ëŠ” ê±°ë¦¬ë¥¼ ìµœì†Œí™”í•˜ê³ , ë‹¤ë¥¸ Classì˜ ë°ì´í„°ëŠ” ê±°ë¦¬ë¥¼ ëŠ˜ë¦¬ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµë©ë‹ˆë‹¤.

```python
# Twin Network
class TwinNetwork(nn.Module):
    def __init__(self):
        super(TwinNetwork, self).__init__()
        self.shared_layers = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(2))

    def forward(self, x):
        return self.shared_layers(x)

# Siamese Network
class SiameseNetwork(nn.Module):
    def __init__(self):
        super(SiameseNetwork, self).__init__()
        self.twin_network = TwinNetwork()

    def forward(self, input1, input2):
        output1 = self.twin_network(input1)
        output2 = self.twin_network(input2)
        return output1, output2
```

- Siamese Networks ì¥ì 

  ê° í´ë˜ìŠ¤ì˜ ë°ì´í„° ê°œìˆ˜ê°€ ì ì–´ë„ í•™ìŠµì´ ê°€ëŠ¥

  ë¶ˆê· í˜•í•œ ë°ì´í„°ë¡œë„ í•™ìŠµ ê°€ëŠ¥

- Siamese Networks ë‹¨ì 

  ë°ì´í„° pair ìƒì„±ìœ¼ë¡œ ì¸í•´ training ë°ì´í„° ìˆ˜ê°€ ë§ì•„ì§ˆ ìˆ˜ ìˆìŒ

  íŠ¹ì • taskì— ì í•©í•œ ëª¨ë¸ì´ ë‹¤ë¥¸ taskì— ì¼ë°˜í™”í•˜ê¸° ì–´ë ¤ì›€

  Input ë°ì´í„°ì˜ ë³€í˜•ì— ë¯¼ê°í•¨


<img src="https://github.com/user-attachments/assets/d54df74e-8f8c-4d23-9985-a40c948c2ee7" width=500>

<img src="https://github.com/user-attachments/assets/e91bfd4f-1db7-4727-8c60-1b84a3b2a66f" width=300>


Loss Functions

1. Contrastive Loss

   Contrastive LossëŠ” ì´ë¯¸ì§€ pairs ì‚¬ì´ì˜ ì°¨ì´ë¥¼ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ Loss

   $ğ¿=ğ‘Œâ‹…ğ·^2+(1âˆ’ğ‘Œ)â‹…max(ğ‘šğ‘ğ‘Ÿğ‘”ğ‘–ğ‘›âˆ’ğ·,0)^2$

   $Where:$
   
   - $D:$ ì´ë¯¸ì§€ features ì‚¬ì´ì˜ ê±°ë¦¬

   - $margin:$ ë‹¤ë¥¸ í´ë˜ìŠ¤ ê°„ì˜ ìµœì†Œ ê±°ë¦¬ ê¸°ì¤€

   - ğ‘Œ: ë‘ ìƒ˜í”Œì˜ ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë ˆì´ë¸” ($Y = 1$ : ê°™ì€ í´ë˜ìŠ¤, $Y = 0$ : ë‹¤ë¥¸ í´ë˜ìŠ¤)

   íŠ¹ì§•:
   
   - ê°™ì€ Classì˜ ìƒ˜í”Œ: ê±°ë¦¬ Dë¥¼ ìµœì†Œí™”
      
   - ë‹¤ë¥¸ Classì˜ ìƒ˜í”Œ: ê±°ë¦¬ë¥¼ margin ì´ìƒìœ¼ë¡œ ë²Œë¦¼
  
<img src="https://github.com/user-attachments/assets/5264586d-d74a-44fe-9f17-f4565b30b215" width=500>


2. Triplet Loss
   
   Triplet LossëŠ” Anchor, Positive, Negativeë¡œ ì´ë£¨ì–´ì§„ tripletì„ ì‚¬ìš©í•˜ì—¬, Anchorì™€ Positive ê±°ë¦¬ë¥¼ ìµœì†Œí™”, Anchorì™€ Negative ê±°ë¦¬ë¥¼ ìµœëŒ€í™”í•˜ê²Œ í•™ìŠµ

   $L=max(d(a,n)âˆ’d(a,p)+margin,0)$

   $Where:$
   
   - $d(a,p):$ Anchor-Positive ê±°ë¦¬

   - $d(a,n):$ Anchor-Negative ê±°ë¦¬

   - $margin:$ ê±°ë¦¬ ê¸°ì¤€

   - $Anchor:$ Triplet Lossì—ì„œ ì°¸ì¡° ì—­í• ì„ í•˜ëŠ” Input Sample

   - $Positive:$ Anchorì™€ ê°™ì€ í´ë˜ìŠ¤ì— ì†í•˜ëŠ” ë°ì´í„°.
  
   - $Negative:$ Anchorì™€ ë‹¤ë¥¸ í´ë˜ìŠ¤ì— ì†í•˜ëŠ” ë°ì´í„°.

<img src="https://github.com/user-attachments/assets/5e0ac3d3-52d0-41af-9ccf-6862098b913d" width=500>
     
     Anchor: "A"ë¼ëŠ” ë°ì´í„°
     
     Positive: ë‹¤ë¥¸ "A"ì˜ ë°ì´í„° (ê°™ì€ í´ë˜ìŠ¤)
     
     Negative: "B"ë¼ëŠ” ë°ì´í„° (ë‹¤ë¥¸ í´ë˜ìŠ¤)

</details>



Self-Supervised Learningì€ ì£¼ë¡œ Siamese Networks êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. Siamese Networksì€ weightë¥¼ ì„œë¡œ ê³µìœ í•˜ëŠ” Twin Networks êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆëŠ”ë°, ì´ë“¤ì€ ê° entitiyë¥¼ ë¹„êµí•˜ëŠ” ë°ì— ìœ ìš©í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê·¸ëŸ¬ë‚˜ Siamese NetworksëŠ” Collapsing(ëª¨ë“  Outputì´ ì¼ì •í•œ ê°’ìœ¼ë¡œ ìˆ˜ë ´í•˜ëŠ” í˜„ìƒ)ì´ ë°œìƒí•˜ëŠ” ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì´ í˜„ìƒì„ í•´ê²°í•˜ê¸° ìœ„í•´ ê¸°ì¡´ì— ë‹¤ìŒì˜ ì—°êµ¬ë“¤ì´ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.



## 2. Related Work
| **Method**          | **Approach**                                                                 | **Key Component**                                                                 |
|----------------------|-----------------------------------------------------------------------------|----------------------------------------------------------------------------------|
| **Contrastive Learning** | SimCLR: Positive PairëŠ” ëŒì–´ë‹¹ê¸°ê³  Negative PairëŠ” ë°€ì–´ë‚´ë„ë¡ í•™ìŠµ                  | Negative Pairsë¡œ Constant Outputì´ Solution Spaceì— í¬í•¨ë˜ì§€ ì•Šë„ë¡ ë°©ì§€          |
| **Clustering**       | SwAV: Siamese Networksì— Online Clusteringì„ ë„ì…                             | Online Clusteringìœ¼ë¡œ Collapsing ë°©ì§€                                            |
| **BYOL**             | Positive Pairsë§Œ ì‚¬ìš©                                                      | Momentum Encoderë¥¼ ì‚¬ìš©í•˜ì—¬ Collapsing ë°©ì§€                                      |

<img src="https://github.com/user-attachments/assets/fa34ddd5-4d2b-443a-9073-240b45fa3ae9" width=400>

| Comparison on Siamese architectures

### How SimSiam Emerges

SimSiamì€ ê¸°ì¡´ ë°©ë²•ë¡ ì—ì„œ Key Component ì œê±°í•˜ì—¬ ë” ê°„ê²°í•œ êµ¬ì¡°ë¥¼ ê°–ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤

### SimSiam: Simplified by Removing Key Components
| **Method**       | **Key Component Removed**         | **Result Model**                  |
|-------------------|-----------------------------------|---------------------------------------|
| **SimCLR**       | âŒ Negative Pairs                 | SimCLR without Negative Pairs      |
| **SwAV**         | âŒ Online Clustering             | SwAV without Online Clustering     |
| **BYOL**         | âŒ Momentum Encoder              | BYOL without Momentum Encoder      |
| **SimSiam**      | â• Adds Stop-Gradient              | SimSiam with Stop-Gradient         |

## 3. Method

<img src="https://github.com/user-attachments/assets/8541e2ab-40ca-42e3-8e98-3d5ec6a6683a" width=400>

| SimSiam Architecture

### 3.1 SimSiam Architecture

1. ë‘ augmentation $x_1, x_2$ â†’ ë™ì¼í•œ ì¸ì½”ë” $f$ í†µê³¼
2. $f:$ ë°±ë³¸ + Projection MLP
3. $h:$ predictor â†’ í•œìª½ì—ë§Œ ì ìš©
4. Stop-gradient â†’ ë‹¤ë¥¸ í•œìª½ì— ì ìš©

### 3.2 Loss
#### 3.2.1 Negative Cosine Similarity

$D(p_1, z_2) = - \frac{p_1}{\||p_1\||_2} \cdot \frac{z_2}{\||z_2\||_2}$

$Where:$

- $p_1$: Predictor MLPì˜ ì¶œë ¥ ë²¡í„°
- $z_2$: Projection MLPì˜ ì¶œë ¥ ë²¡í„° (stop-gradient ì ìš©)
- $\||p_1\||_2$: $p_1$ ë²¡í„°ì˜ $\ell_2$-ë…¸ë¦„(norm)
- $\||z_2\||_2$: $z_2$ ë²¡í„°ì˜ $\ell_2$-ë…¸ë¦„(norm)

#### 3.2.2 Symmetrized Loss

Symmetrized LossëŠ” ë‘ê°œì˜ Augmentationì˜ ì†ì‹¤ì„ ëŒ€ì¹­ì ìœ¼ë¡œ ê³„ì‚°í•˜ì—¬ í•™ìŠµ ì•ˆì •ì„±ì„ ë¶€ì—¬

$L = \frac{1}{2} D(p_1, stopgrad(z_2)) + \frac{1}{2} D(p_2, stopgrad(z_1))$

$Where:$

- $D(p_1,z_2)$: ë‘ ë²¡í„° $p_1$, $z_2$ ê°„ì˜ Negative Cosine Similarity
- $p_1, p_2$: Predictor MLPì˜ ì¶œë ¥ ë²¡í„°
- $z_1, z_2$: Projection MLPì˜ ì¶œë ¥ ë²¡í„°
- $stopgrad(z)$: Stop-gradient ì—°ì‚°ì´ ì ìš©ëœ z, ìƒìˆ˜ë¡œ ì·¨ê¸‰ ë˜ì–´ Gradient ì „íŒŒë˜ì§€ ì•ŠëŠ” í…ì„œ

#### 3.2.3 Stop-Gradient
Stop-gradient ì—°ì‚°ì€ collapsing(ì¶œë ¥ì´ ì¼ì •í•œ ê°’ìœ¼ë¡œ ìˆ˜ë ´í•˜ëŠ” í˜„ìƒ)ì„ ë°©ì§€í•˜ëŠ” ë° í•µì‹¬ì ì¸ ì—­í• 

Gradient Flow ì°¨ë‹¨

$stopgrad(z) = z$ ($z$ë¥¼ ìƒìˆ˜ë¡œ ì·¨ê¸‰í•˜ì—¬ ì—­ì „íŒŒì‹œ Gradient ê³„ì‚°í•˜ì§€ ì•Šë„ë¡)

forward ì‹œ ê°’ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©, backwardì—ì„œëŠ” $\frac{âˆ‚stopgrad(z)}{âˆ‚z} = 0$

#### 3.2.4 SimSiam ë™ì‘ ì›ë¦¬

1. í•˜ë‚˜ì˜ Input image $x$ì— ëŒ€í•´ random augmentationìœ¼ë¡œ augmentation $x_1$, $x_2$ ìƒì„±

2. augmentation $x_1$, $x_2$ëŠ” Encoder $f$ë¥¼ í†µê³¼ (ì´ë–„, ë‘ EncoderëŠ” weightì„ ê³µìœ )

3. Encoderë¥¼ í†µê³¼í•œ ë‘ Vectore ì¤‘ í•œìª½ì—ë§Œ Predictor $h$ë¥¼ í†µê³¼í•´ ìƒˆë¡œìš´ vector $z$ë¥¼ ë§Œë“ ë‹¤.

   $p_1 = h(f(x_1))$

   $z_2 = f(x_2)$

4. Symmetrized Loss
   - augmenatation $x1$ì—ì„œ ë‚˜ì˜¨ $p_1$ê³¼ $z_2$ê°„ ì†ì‹¤ ê³„ì‚°
   - augmenatation $x2$ì—ì„œ ë‚˜ì˜¨ $p_2$ê³¼ $z_1$ê°„ ì†ì‹¤ ê³„ì‚°
     (ë‘ ì†ì‹¤ì„ í•©ì‚°í•˜ê³  í‰ê· ì„ ë‚´ì„œ ìµœì¢… ì†ì‹¤ë¡œ ì‚¬ìš©)
5. Stop-gradient
   - ì²« ë²ˆì§¸ í•­ì—ì„œëŠ” $z_2$ë¥¼ ìƒìˆ˜ë¡œ ì·¨ê¸‰í•˜ì—¬ gradient $z_2$ë¡œ ì „ë‹¬ë˜ì§€ ì•ŠìŒ
   - ë‘ ë²ˆì§¸ í•­ì—ì„œëŠ” $z_1$ì— stop-gradientê°€ ì ìš©
   - ë‘ augmentationì´ í•™ìŠµ ê³¼ì •ì—ì„œ ê· í˜•ì„ ì´ë£¨ë„ë¡ í•¨
6. Loss Symmetry
   - ë‘ augmentationì´ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµì— ê¸°ì—¬í•˜ë¯€ë¡œ í•œìª½ ë„¤íŠ¸ì›Œí¬ê°€ ê³¼ë„í•˜ê²Œ í•™ìŠµë˜ì§€ ì•Šë„ë¡ ë°©ì§€
   - ëª¨ë¸ì´ ì–‘ìª½ ì…ë ¥ì— ëŒ€í•´ ê· í˜• ì¡íŒ í‘œí˜„ì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë„ì›€


## 4. Empirical Study
### 4.1 Stop-Gradientì˜ ì—­í• 
   -  Stop-gradientê°€ ì—†ìœ¼ë©´ Collapsing ë¬¸ì œê°€ ë°œìƒ
   -  Stop-graidentëŠ” Collapsingì„ ë°©ì§€í•˜ë©°, $z$ê°’ì˜ ë¶„ì‚°ì´ ìœ ì§€
   -  ë‹¨ìœ„ë²¡í„° $\frac\{z}{||p_1\||_2}$ ì˜ ì±„ë„ë³„ í‘œì¤€í¸ì°¨(std)ë¥¼ ê³„ì‚°í•˜ì—¬ Collapsing ì—¬ë¶€ í‰ê°€

1) Collapsing ë°œìƒ ì‹œ
   
   $z$ê°€ ìƒìˆ˜ë¡œ ìˆ˜ë ´í•˜ë©°, ì±„ë„ë³„ í‘œì¤€í¸ì°¨ê°€ 0ì— ê°€ê¹Œì›Œì§‘ë‹ˆë‹¤.

3) Gaussian ë¶„í¬ì— ë”°ë¥¸ ê²½ìš°
   
   $z$ê°€ Unit Hypersphere ìœ„ì— ê³ ë¥´ê²Œ ë¶„í¬ë˜ë©°, í‘œì¤€í¸ì°¨ëŠ” íŠ¹ì • ê°’ì— ê·¼ì ‘í•©ë‹ˆë‹¤.

   $std â‰ˆ \frac{1}{\sqrt{d}}$  ($d$ëŠ” $z$ ë²¡í„°ì˜ ì°¨ì›)

---

### 4.2 Predictor

- **Predictorì˜ ì—­í• **:  
  - Predictor $h$ëŠ” Projection MLPì˜ ì¶œë ¥ $z$ë¥¼ ë‹¤ë¥¸ augmentationì˜ $z$ì™€ ì •ë ¬ë˜ë„ë¡ í•™ìŠµ 
  - í‘œí˜„ì„ ìµœì‹  ìƒíƒœë¡œ ìœ ì§€í•˜ë©°, augmentation ë¶„í¬ì˜ ê¸°ëŒ€ê°’ì„ ê·¼ì‚¬í•˜ëŠ” ë° ë„ì›€

- **ì‹¤í—˜ ê²°ê³¼**:  
  - **Predictor ì œê±°**:  
    - $h$ë¥¼ ì œê±°í•˜ë©´ ëª¨ë¸ì´ collapsing(ì¶œë ¥ì´ ìƒìˆ˜ë¡œ ìˆ˜ë ´)í•˜ì—¬ í•™ìŠµ ë¶ˆê°€
  - **ê³ ì •ëœ Predictor**:  
    - ë¬´ì‘ìœ„ ì´ˆê¸°í™” ìƒíƒœë¡œ ê³ ì •ëœ $h$ëŠ” í•™ìŠµí•˜ì§€ ëª»í•˜ë©°, ì†ì‹¤ì´ ë†’ìŒ
  - **í•™ìŠµ ê°€ëŠ¥í•œ Predictor**:  
    - ë™ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ëŠ” $h$ëŠ” SimSiamì˜ ì•ˆì •ì  í•™ìŠµì„ ë„ì›€

---

### 4.3 Batch Size

- **íš¨ê³¼**:  
  - SimSiamì€ ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸°(64~4096)ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ì‘ë™
  - ë°°ì¹˜ í¬ê¸°ê°€ ì‘ì•„ë„ ì„±ëŠ¥ ì €í•˜ê°€ ì—†ìŒ

- **SimCLR ë° SwAVì™€ì˜ ì°¨ì´ì **:  
  - SimCLRì™€ SwAVëŠ” í° ë°°ì¹˜ í¬ê¸°(4096 ì´ìƒ)ê°€ í•„ìš”í•˜ì§€ë§Œ, SimSiamì€ ì¼ë°˜ì ì¸ í¬ê¸°(512)ì—ì„œë„ ì‘ë™

---

### 4.4 Batch Normalization

- **íš¨ê³¼**:  
  - Batch Normalization(BN)ì€ ê° ë ˆì´ì–´ì˜ ì¶œë ¥ ë¶„í¬ë¥¼ ì•ˆì •í™”í•˜ì—¬ ìµœì í™”
  - SimSiamì—ì„œ BNì€ collapsing ë°©ì§€ì—ëŠ” ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠìŒ  

- **êµ¬ì„±**:  
  - **Projection MLP**:  
    - BNì€ ëª¨ë“  FC ë ˆì´ì–´ì— ì ìš©ë˜ë©°, ì¶œë ¥ FCì—ëŠ” ReLUê°€ ì—†ìŒ
  - **Prediction MLP**:  
    - BNì€ ì€ë‹‰ FC ë ˆì´ì–´ì— ì ìš©ë˜ì§€ë§Œ, ì¶œë ¥ FCì—ëŠ” ì ìš©ë˜ì§€ ì•ŠìŒ

---

### 4.5 Symmetrization

- **ìˆ˜ì‹**:  
  $$L = \frac{1}{2} D(p_1, stopgrad(z_2)) + \frac{1}{2} D(p_2, stopgrad(z_1))$$  
  - ë‘ augmentation ê°„ ì†ì‹¤ì„ ëŒ€ì¹­ì ìœ¼ë¡œ ê³„ì‚°í•˜ì—¬ í•™ìŠµ ì•ˆì •ì„±ì„ ë†’ì„

- **íš¨ê³¼**:  
  - ëŒ€ì¹­í™”ëŠ” ì •í™•ë„ë¥¼ ë†’ì´ëŠ” ë° ê¸°ì—¬í•˜ì§€ë§Œ, collapsing ë°©ì§€ì—ëŠ” í•„ìˆ˜ì ì´ì§€ ì•ŠìŒ

- **ë¹„êµ ì‹¤í—˜**:  
  - ëŒ€ì¹­í™” ì œê±°(ë¹„ëŒ€ì¹­ ì†ì‹¤)ë¥¼ ì ìš©í•´ë„ ëª¨ë¸ì´ collapsing ì—†ì´ í•™ìŠµí•  ìˆ˜ ìˆì§€ë§Œ, ì •í™•ë„ëŠ” ì•½ê°„ ê°ì†Œ
