# SimSiam

"Exploring Simple Siamese Representation Learning" - 2020

-Xinlei Chen, Kaiming He (Facebook AI Research)

https://arxiv.org/pdf/2011.10566

---

## 1. Introduction

### Siamese Networks

<details> <summary>Siamese Networks ë³´ê¸°</summary>
 
 DeepLearningì—ì„œëŠ” í•™ìŠµì„ ìœ„í•´ ë§ì€ ì–‘ì˜ ë°ì´í„°ë¥¼ í•„ìš”ë¡œ í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë‹¤ëŠ” ë§ì€, DeepLearning ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•ŠìŒì„ ì•”ì‹œí•©ë‹ˆë‹¤.

 ê·¸ë˜ì„œ ê³ ì•ˆëœ Siamese NetworksëŠ” ë°ì´í„° ì–‘ì´ ì ê±°ë‚˜, Imbalanced Class Distributioní•œ ë°ì´í„°ì—ì„œë„ ëª¨ë¸ì˜ ì •í™•ì„±ì„ ë†’í ìˆ˜ ìˆìŠµë‹ˆë‹¤.
 
 Siamese NetworksëŠ” ë™ì¼í•œ parametersë‚˜ weightsì„ ê³µìœ í•˜ëŠ” Twin Networksë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.
 
 ì´ ë„¤íŠ¸ì›Œí¬ëŠ” í•œ ìŒì˜ inputsë¥¼ ë°›ì•„ ê°ê°ì˜ featuresë¥¼ ì¶”ì¶œí•œ ë’¤ ë‘ inputs ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Classificationì„ ìˆ˜í–‰í•˜ë©°, ê°™ì€ Classì˜ ë°ì´í„°ëŠ” ê±°ë¦¬ë¥¼ ìµœì†Œí™”í•˜ê³ , ë‹¤ë¥¸ Classì˜ ë°ì´í„°ëŠ” ê±°ë¦¬ë¥¼ ëŠ˜ë¦¬ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµë©ë‹ˆë‹¤.

```python
# Twin Network
class TwinNetwork(nn.Module):
    def __init__(self):
        super(TwinNetwork, self).__init__()
        self.shared_layers = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=3),
            nn.ReLU(),
            nn.MaxPool2d(2))

    def forward(self, x):
        return self.shared_layers(x)

# Siamese Network
class SiameseNetwork(nn.Module):
    def __init__(self):
        super(SiameseNetwork, self).__init__()
        self.twin_network = TwinNetwork()

    def forward(self, input1, input2):
        output1 = self.twin_network(input1)
        output2 = self.twin_network(input2)
        return output1, output2
```

- Siamese Networks ì¥ì 

  ê° í´ë˜ìŠ¤ì˜ ë°ì´í„° ê°œìˆ˜ê°€ ì ì–´ë„ í•™ìŠµì´ ê°€ëŠ¥

  ë¶ˆê· í˜•í•œ ë°ì´í„°ë¡œë„ í•™ìŠµ ê°€ëŠ¥

- Siamese Networks ë‹¨ì 

  ë°ì´í„° pair ìƒì„±ìœ¼ë¡œ ì¸í•´ training ë°ì´í„° ìˆ˜ê°€ ë§ì•„ì§ˆ ìˆ˜ ìˆìŒ

  íŠ¹ì • taskì— ì í•©í•œ ëª¨ë¸ì´ ë‹¤ë¥¸ taskì— ì¼ë°˜í™”í•˜ê¸° ì–´ë ¤ì›€

  Input ë°ì´í„°ì˜ ë³€í˜•ì— ë¯¼ê°í•¨


<img src="https://github.com/user-attachments/assets/d54df74e-8f8c-4d23-9985-a40c948c2ee7" width=500>

<img src="https://github.com/user-attachments/assets/e91bfd4f-1db7-4727-8c60-1b84a3b2a66f" width=300>


Loss Functions

1. Contrastive Loss

   Contrastive LossëŠ” ì´ë¯¸ì§€ pairs ì‚¬ì´ì˜ ì°¨ì´ë¥¼ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ Loss

   $ğ¿=ğ‘Œâ‹…ğ·^2+(1âˆ’ğ‘Œ)â‹…max(ğ‘šğ‘ğ‘Ÿğ‘”ğ‘–ğ‘›âˆ’ğ·,0)^2$

   $Where:$
   
   - $D:$ ì´ë¯¸ì§€ features ì‚¬ì´ì˜ ê±°ë¦¬

   - $margin:$ ë‹¤ë¥¸ í´ë˜ìŠ¤ ê°„ì˜ ìµœì†Œ ê±°ë¦¬ ê¸°ì¤€

   - ğ‘Œ: ë‘ ìƒ˜í”Œì˜ ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë ˆì´ë¸” ($Y = 1$ : ê°™ì€ í´ë˜ìŠ¤, $Y = 0$ : ë‹¤ë¥¸ í´ë˜ìŠ¤)

   íŠ¹ì§•:
   
   - ê°™ì€ Classì˜ ìƒ˜í”Œ: ê±°ë¦¬ Dë¥¼ ìµœì†Œí™”
      
   - ë‹¤ë¥¸ Classì˜ ìƒ˜í”Œ: ê±°ë¦¬ë¥¼ margin ì´ìƒìœ¼ë¡œ ë²Œë¦¼
  
<img src="https://github.com/user-attachments/assets/5264586d-d74a-44fe-9f17-f4565b30b215" width=500>


2. Triplet Loss
   
   Triplet LossëŠ” Anchor, Positive, Negativeë¡œ ì´ë£¨ì–´ì§„ tripletì„ ì‚¬ìš©í•˜ì—¬, Anchorì™€ Positive ê±°ë¦¬ë¥¼ ìµœì†Œí™”, Anchorì™€ Negative ê±°ë¦¬ë¥¼ ìµœëŒ€í™”í•˜ê²Œ í•™ìŠµ

   $L=max(d(a,n)âˆ’d(a,p)+margin,0)$

   $Where:$
   
   - $d(a,p):$ Anchor-Positive ê±°ë¦¬

   - $d(a,n):$ Anchor-Negative ê±°ë¦¬

   - $margin:$ ê±°ë¦¬ ê¸°ì¤€

   - $Anchor:$ Triplet Lossì—ì„œ ì°¸ì¡° ì—­í• ì„ í•˜ëŠ” Input Sample

   - $Positive:$ Anchorì™€ ê°™ì€ í´ë˜ìŠ¤ì— ì†í•˜ëŠ” ë°ì´í„°.
  
   - $Negative:$ Anchorì™€ ë‹¤ë¥¸ í´ë˜ìŠ¤ì— ì†í•˜ëŠ” ë°ì´í„°.

<img src="https://github.com/user-attachments/assets/5e0ac3d3-52d0-41af-9ccf-6862098b913d" width=500>
     
     Anchor: "A"ë¼ëŠ” ë°ì´í„°
     
     Positive: ë‹¤ë¥¸ "A"ì˜ ë°ì´í„° (ê°™ì€ í´ë˜ìŠ¤)
     
     Negative: "B"ë¼ëŠ” ë°ì´í„° (ë‹¤ë¥¸ í´ë˜ìŠ¤)

</details>



Self-Supervised Learningì€ ì£¼ë¡œ Siamese Networks êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. Siamese Networksì€ weightë¥¼ ì„œë¡œ ê³µìœ í•˜ëŠ” Twin Networks êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆëŠ”ë°, ì´ë“¤ì€ ê° entitiyë¥¼ ë¹„êµí•˜ëŠ” ë°ì— ìœ ìš©í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê·¸ëŸ¬ë‚˜ Siamese NetworksëŠ” Collapsing(ëª¨ë“  Outputì´ ì¼ì •í•œ ê°’ìœ¼ë¡œ ìˆ˜ë ´í•˜ëŠ” í˜„ìƒ)ì´ ë°œìƒí•˜ëŠ” ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì´ í˜„ìƒì„ í•´ê²°í•˜ê¸° ìœ„í•´ ê¸°ì¡´ì— ë‹¤ìŒì˜ ì—°êµ¬ë“¤ì´ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.



## 2. Related Work
| **Method**          | **Approach**                                                                 | **Key Component**                                                                 |
|----------------------|-----------------------------------------------------------------------------|----------------------------------------------------------------------------------|
| **Contrastive Learning** | SimCLR: Positive PairëŠ” ëŒì–´ë‹¹ê¸°ê³  Negative PairëŠ” ë°€ì–´ë‚´ë„ë¡ í•™ìŠµ                  | Negative Pairsë¡œ Constant Outputì´ Solution Spaceì— í¬í•¨ë˜ì§€ ì•Šë„ë¡ ë°©ì§€          |
| **Clustering**       | SwAV: Siamese Networksì— Online Clusteringì„ ë„ì…                             | Online Clusteringìœ¼ë¡œ Collapsing ë°©ì§€                                            |
| **BYOL**             | Positive Pairsë§Œ ì‚¬ìš©                                                      | Momentum Encoderë¥¼ ì‚¬ìš©í•˜ì—¬ Collapsing ë°©ì§€                                      |

<img src="https://github.com/user-attachments/assets/fa34ddd5-4d2b-443a-9073-240b45fa3ae9" width=400>

| Comparison on Siamese architectures

### How SimSiam Emerges

SimSiamì€ ê¸°ì¡´ ë°©ë²•ë¡ ì—ì„œ Key Component ì œê±°í•˜ì—¬ ë” ê°„ê²°í•œ êµ¬ì¡°ë¥¼ ê°–ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤

### SimSiam: Simplified by Removing Key Components
| **Method**       | **Key Component Removed**         | **Result Model**                  |
|-------------------|-----------------------------------|---------------------------------------|
| **SimCLR**       | âŒ Negative Pairs                 | SimCLR without Negative Pairs      |
| **SwAV**         | âŒ Online Clustering             | SwAV without Online Clustering     |
| **BYOL**         | âŒ Momentum Encoder              | BYOL without Momentum Encoder      |
| **SimSiam**      | â• Adds Stop-Gradient              | SimSiam with Stop-Gradient         |

## 3. Method

<img src="https://github.com/user-attachments/assets/8541e2ab-40ca-42e3-8e98-3d5ec6a6683a" width=400>

| SimSiam Architecture

SimSiamì€ ìê°€ ì§€ë„ í•™ìŠµì—ì„œ Collapsing í˜„ìƒ(ì¶œë ¥ì´ ì¼ì •í•œ ê°’ìœ¼ë¡œ ìˆ˜ë ´)ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ë‹¨ìˆœí•œ êµ¬ì¡°ì…ë‹ˆë‹¤. Stop-gradientë¼ëŠ” ë…íŠ¹í•œ ê¸°ë²•ì„ í™œìš©í•˜ì—¬ Negative Pair ì—†ì´ë„ í•™ìŠµì„ ì•ˆì •ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 3.1 Stop-Gradient

1. Collapsing ë°©ì§€
  
   - ëª¨ë“  ì¶œë ¥ì´ ì¼ì •í•œ ê°’ìœ¼ë¡œ ìˆ˜ë ´í•˜ë©´ ëª¨ë¸ì´ ë” ì´ìƒ ì˜ë¯¸ ìˆëŠ” í‘œí˜„ì„ í•™ìŠµí•˜ì§€ ëª»í•©ë‹ˆë‹¤.

   - Stop-gradientëŠ” í•˜ë‚˜ì˜ ë„¤íŠ¸ì›Œí¬ ì¶œë ¥($z1$ ë˜ëŠ” $z2$)ì„ ìƒìˆ˜ì²˜ëŸ¼ ì·¨ê¸‰í•˜ì—¬ ì—­ì „íŒŒë¥¼ ì°¨ë‹¨

2. Gradient Flow ì°¨ë‹¨

   - $stopgrad(z) = z$ ($z$ë¥¼ ìƒìˆ˜ë¡œ ì·¨ê¸‰í•˜ì—¬ ì—­ì „íŒŒì‹œ Gradient ê³„ì‚°í•˜ì§€ ì•Šë„ë¡)

   - forward ì‹œ ê°’ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©, backwardì—ì„œëŠ” $\frac{âˆ‚stopgrad(z)}{âˆ‚z} = 0$

3. Gaussian ë¶„í¬ ìœ ì§€

   - Stop-gradientëŠ” $z$ ê°’ì´ Unit Hypersphere ìœ„ì— ê³ ë¥´ê²Œ ë¶„í¬í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.
   
   - ë‹¨ìœ„ë²¡í„° $\frac\{z}{||p_1\||_2}$ ì˜ ì±„ë„ë³„ í‘œì¤€í¸ì°¨(std)ë¥¼ ê³„ì‚°í•˜ì—¬ Collapsing ì—¬ë¶€ í‰ê°€

---

### 3.2 SimSiam Architecture

1. Input Image $x$ â†’ ëœë¤ augmentationìœ¼ë¡œ $x_1, x_2$ ìƒì„±
2. ë‘ augmentation $x_1, x_2$ â†’ ë™ì¼í•œ ì¸ì½”ë” $f$ í†µê³¼
3. $f:$ ë°±ë³¸ + Projection MLP
4. $h:$ predictor â†’ í•œìª½ì—ë§Œ ì ìš©
5. Stop-gradient â†’ ë‹¤ë¥¸ í•œìª½ì— ì ìš©

---

### 3.3 Loss
### 3.3.1 Negative Cosine Similarity

$D(p_1, z_2) = - \frac{p_1}{\||p_1\||_2} \cdot \frac{z_2}{\||z_2\||_2}$

$Where:$

- $p_1$: Predictor MLPì˜ ì¶œë ¥ ë²¡í„°
- $z_2$: Projection MLPì˜ ì¶œë ¥ ë²¡í„° (stop-gradient ì ìš©)
- $\||p_1\||_2$: $p_1$ ë²¡í„°ì˜ $\ell_2$-ë…¸ë¦„(norm)
- $\||z_2\||_2$: $z_2$ ë²¡í„°ì˜ $\ell_2$-ë…¸ë¦„(norm)

### 3.3.2 Symmetrized Loss

Symmetrized LossëŠ” ë‘ê°œì˜ Augmentationì˜ ì†ì‹¤ì„ ëŒ€ì¹­ì ìœ¼ë¡œ ê³„ì‚°í•˜ì—¬ í•™ìŠµ ì•ˆì •ì„±ì„ ë¶€ì—¬

$L = \frac{1}{2} D(p_1, stopgrad(z_2)) + \frac{1}{2} D(p_2, stopgrad(z_1))$

$Where:$

- $D(p_1,z_2)$: ë‘ ë²¡í„° $p_1$, $z_2$ ê°„ì˜ Negative Cosine Similarity
- $p_1, p_2$: Predictor MLPì˜ ì¶œë ¥ ë²¡í„°
- $z_1, z_2$: Projection MLPì˜ ì¶œë ¥ ë²¡í„°
- $stopgrad(z)$: Stop-gradient ì—°ì‚°ì´ ì ìš©ëœ z, ìƒìˆ˜ë¡œ ì·¨ê¸‰ ë˜ì–´ Gradient ì „íŒŒë˜ì§€ ì•ŠëŠ” í…ì„œ

---

### 3.4 SimSiam ë™ì‘ ì›ë¦¬

1. í•˜ë‚˜ì˜ Input image $x$ì— ëŒ€í•´ random augmentationìœ¼ë¡œ augmentation $x_1$, $x_2$ ìƒì„±

2. augmentation $x_1$, $x_2$ëŠ” Encoder $f$ë¥¼ í†µê³¼ (ì´ë–„, ë‘ EncoderëŠ” weightì„ ê³µìœ )

3. Encoderë¥¼ í†µê³¼í•œ ë‘ Vectore ì¤‘ í•œìª½ì—ë§Œ Predictor $h$ë¥¼ í†µê³¼í•´ ìƒˆë¡œìš´ vector $p$ë¥¼ ë§Œë“ ë‹¤.

   $p_1 = h(f(x_1))$

   $z_2 = f(x_2)$

4. Symmetrized Loss
   - augmenatation $x1$ì—ì„œ ë‚˜ì˜¨ $p_1$ê³¼ $z_2$ê°„ ì†ì‹¤ ê³„ì‚°
   - augmenatation $x2$ì—ì„œ ë‚˜ì˜¨ $p_2$ê³¼ $z_1$ê°„ ì†ì‹¤ ê³„ì‚°
     (ë‘ ì†ì‹¤ì„ í•©ì‚°í•˜ê³  í‰ê· ì„ ë‚´ì„œ ìµœì¢… ì†ì‹¤ë¡œ ì‚¬ìš©)
5. Stop-gradient
   - ì²« ë²ˆì§¸ í•­ì—ì„œëŠ” $z_2$ë¥¼ ìƒìˆ˜ë¡œ ì·¨ê¸‰í•˜ì—¬ gradient $z_2$ë¡œ ì „ë‹¬ë˜ì§€ ì•ŠìŒ
   - ë‘ ë²ˆì§¸ í•­ì—ì„œëŠ” $z_1$ì— stop-gradientê°€ ì ìš©
   - ë‘ augmentationì´ í•™ìŠµ ê³¼ì •ì—ì„œ ê· í˜•ì„ ì´ë£¨ë„ë¡ í•¨
6. Loss Symmetry
   - ë‘ augmentationì´ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµì— ê¸°ì—¬í•˜ë¯€ë¡œ í•œìª½ ë„¤íŠ¸ì›Œí¬ê°€ ê³¼ë„í•˜ê²Œ í•™ìŠµë˜ì§€ ì•Šë„ë¡ ë°©ì§€
   - ëª¨ë¸ì´ ì–‘ìª½ ì…ë ¥ì— ëŒ€í•´ ê· í˜• ì¡íŒ í‘œí˜„ì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë„ì›€

### 3.4.1 ì‹œë‚˜ë¦¬ì˜¤

### 1. **ì´ë¯¸ì§€ Aì™€ ì´ë¯¸ì§€ Bê°€ ë“¤ì–´ê°€ëŠ” ê²½ìš°**

#### **Step 1. Augmentation ì ìš©**

- ì´ë¯¸ì§€ Aì™€ B ê°ê°ì— ëŒ€í•´ ì„œë¡œ ë‹¤ë¥¸ ëœë¤ ë³€í˜•(augmentation)ì„ ì ìš©í•˜ì—¬ ë‘ ê°œì˜ ë·°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

  ì˜ˆë¥¼ ë“¤ì–´:
  - $x_a^{1}$: ì´ë¯¸ì§€ Aì— augmentation 1 ì ìš©
  - $x_a^{2}$: ì´ë¯¸ì§€ Aì— augmentation 2 ì ìš©
  - $x_b^{1}$: ì´ë¯¸ì§€ Bì— augmentation 1 ì ìš©
  - $x_b^{2}$: ì´ë¯¸ì§€ Bì— augmentation 2 ì ìš©

#### **Step 2. Encoderë¥¼ í†µí•´ Feature ì¶”ì¶œ**

- ê°ê°ì˜ augmentation ì´ë¯¸ì§€ë¥¼ Encoder $f$ì— ì…ë ¥í•˜ì—¬ Feature Vector $z$ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
  - $z_a^{1} = f(x_a^{1})$
  - $z_a^{2} = f(x_a^{2})$
  - $z_b^{1} = f(x_b^{1})$
  - $z_b^{2} = f(x_b^{2})$

#### **Step 3. Projection MLPë¥¼ í†µê³¼**

- ì¶”ì¶œëœ Feature Vector $z$ë¥¼ Projection MLPì— ì…ë ¥í•˜ì—¬ ê³ ì°¨ì› í‘œí˜„ ê³µê°„ìœ¼ë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.
  - $z_a^{1}, z_a^{2} \rightarrow \text{Projection MLP} \rightarrow z_a'^{1}, z_a'^{2}$
  - $z_b^{1}, z_b^{2} \rightarrow \text{Projection MLP} \rightarrow z_b'^{1}, z_b'^{2}$

#### **Step 4. Predictorë¥¼ í†µí•´ ì •ë ¬**

- Projection MLPì˜ ì¶œë ¥ì„ Predictor $h$ì— ì…ë ¥í•˜ì—¬ ë‹¤ë¥¸ ë·°ì™€ ì •ë ¬í•˜ë„ë¡ í•™ìŠµí•©ë‹ˆë‹¤.
  - $p_a^{1} = h(z_a'^{1}), p_a^{2} = h(z_a'^{2})$
  - $p_b^{1} = h(z_b'^{1}), p_b^{2} = h(z_b'^{2})$

#### **Step 5. Symmetrized Loss ê³„ì‚°**

- ì´ë¯¸ì§€ Aì™€ B ê°ê°ì˜ augmentation ìŒì— ëŒ€í•´ ëŒ€ì¹­ ì†ì‹¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
  - ì´ë¯¸ì§€ A
  
    $L_A = \frac{1}{2} D(p_a^{1}, \text{stopgrad}(z_a'^{2})) + \frac{1}{2} D(p_a^{2}, \text{stopgrad}(z_a'^{1}))$
    
  - ì´ë¯¸ì§€ B
    
    $L_B = \frac{1}{2} D(p_b^{1}, \text{stopgrad}(z_b'^{2})) + \frac{1}{2} D(p_b^{2}, \text{stopgrad}(z_b'^{1}))$

#### **Step 6. ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸**

- $L_A + L_B$ì˜ ì´ ì†ì‹¤ì„ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

- ì „ì²´ ê³¼ì •ì€ positive pair ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•˜ëŠ” ë° ì§‘ì¤‘í•˜ë©°, ì´ë¯¸ì§€ì˜ ê°œìˆ˜ì™€ ê´€ê³„ì—†ì´ ë³‘ë ¬ ì²˜ë¦¬ê°€ ê°€ëŠ¥

---

### 2. **ì´ë¯¸ì§€ Cê°€ ì¶”ê°€ë¡œ ë“¤ì–´ê°€ëŠ” ê²½ìš°**

ì´ë¯¸ì§€ Cê°€ ì¶”ê°€ë¡œ ë“¤ì–´ì˜¤ë©´, ë™ì¼í•œ ê³¼ì •ì„ ë°˜ë³µí•˜ì§€ë§Œ ì†ì‹¤ ê³„ì‚°ì€ A, B, C ê°ê°ì˜ augmentationì— ëŒ€í•´ ë…ë¦½ì ìœ¼ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤.

#### **Step 1 - 4: ë™ì¼í•œ ê³¼ì • ë°˜ë³µ**

- $x_c^{1}, x_c^{2}$ ì— augmentation ì ìš©
- Encoderì™€ Projection MLP, Predictorë¥¼ í†µí•´ $p_c^{1}, p_c^{2}$ ìƒì„±

#### **Step 5. Symmetrized Loss ê³„ì‚° (C í¬í•¨)**

ì´ë¯¸ì§€ A, B, C ê°ê°ì— ëŒ€í•´ ê°œë³„ì ìœ¼ë¡œ Lossë¥¼ ê³„ì‚°:
- ì´ë¯¸ì§€ A
  
  $L_A = \frac{1}{2} D(p_a^{1}, \text{stopgrad}(z_a'^{2})) + \frac{1}{2} D(p_a^{2}, \text{stopgrad}(z_a'^{1}))$
  
- ì´ë¯¸ì§€ B
  
  $L_B = \frac{1}{2} D(p_b^{1}, \text{stopgrad}(z_b'^{2})) + \frac{1}{2} D(p_b^{2}, \text{stopgrad}(z_b'^{1}))$

- ì´ë¯¸ì§€ C
  
  $L_C = \frac{1}{2} D(p_c^{1}, \text{stopgrad}(z_c'^{2})) + \frac{1}{2} D(p_c^{2}, \text{stopgrad}(z_c'^{1}))$

#### **Step 6. ì´ ì†ì‹¤ ê³„ì‚°**

- ìµœì¢… ì†ì‹¤
  
  $L_{\text{total}} = L_A + L_B + L_C$

#### **Step 7. ëª¨ë¸ ì—…ë°ì´íŠ¸**

- ì´ ì†ì‹¤ $L_{\text{total}}$ ì„ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸

- ì „ì²´ ê³¼ì •ì€ positive pair ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•˜ëŠ” ë° ì§‘ì¤‘í•˜ë©°, ì´ë¯¸ì§€ì˜ ê°œìˆ˜ì™€ ê´€ê³„ì—†ì´ ë³‘ë ¬ ì²˜ë¦¬ê°€ ê°€ëŠ¥


---

## 4. Empirical Study
### 4.1 Stop-Gradientì˜ ì—­í• 
   -  Stop-gradientê°€ ì—†ìœ¼ë©´ Collapsing ë¬¸ì œê°€ ë°œìƒ
   -  Stop-graidentëŠ” Collapsingì„ ë°©ì§€í•˜ë©°, $z$ê°’ì˜ ë¶„ì‚°ì´ ìœ ì§€
   -  ë‹¨ìœ„ë²¡í„° $\frac\{z}{||p_1\||_2}$ ì˜ ì±„ë„ë³„ í‘œì¤€í¸ì°¨(std)ë¥¼ ê³„ì‚°í•˜ì—¬ Collapsing ì—¬ë¶€ í‰ê°€

1) Collapsing ë°œìƒ ì‹œ
   
   $z$ê°€ ìƒìˆ˜ë¡œ ìˆ˜ë ´í•˜ë©°, ì±„ë„ë³„ í‘œì¤€í¸ì°¨ê°€ 0ì— ê°€ê¹Œì›Œì§‘ë‹ˆë‹¤.

3) Gaussian ë¶„í¬ì— ë”°ë¥¸ ê²½ìš°
   
   $z$ê°€ Unit Hypersphere ìœ„ì— ê³ ë¥´ê²Œ ë¶„í¬ë˜ë©°, í‘œì¤€í¸ì°¨ëŠ” íŠ¹ì • ê°’ì— ê·¼ì ‘í•©ë‹ˆë‹¤.

   $std â‰ˆ \frac{1}{\sqrt{d}}$  ($d$ëŠ” $z$ ë²¡í„°ì˜ ì°¨ì›)

---

### 4.2 Predictor

- **Predictorì˜ ì—­í• **:  
  - Predictor $h$ëŠ” Projection MLPì˜ ì¶œë ¥ $z$ë¥¼ ë‹¤ë¥¸ augmentationì˜ $z$ì™€ ì •ë ¬ë˜ë„ë¡ í•™ìŠµ 
  - í‘œí˜„ì„ ìµœì‹  ìƒíƒœë¡œ ìœ ì§€í•˜ë©°, augmentation ë¶„í¬ì˜ ê¸°ëŒ€ê°’ì„ ê·¼ì‚¬í•˜ëŠ” ë° ë„ì›€

- **ì‹¤í—˜ ê²°ê³¼**:  
  - **Predictor ì œê±°**:  
    - $h$ë¥¼ ì œê±°í•˜ë©´ ëª¨ë¸ì´ collapsing(ì¶œë ¥ì´ ìƒìˆ˜ë¡œ ìˆ˜ë ´)í•˜ì—¬ í•™ìŠµ ë¶ˆê°€
  - **ê³ ì •ëœ Predictor**:  
    - ë¬´ì‘ìœ„ ì´ˆê¸°í™” ìƒíƒœë¡œ ê³ ì •ëœ $h$ëŠ” í•™ìŠµí•˜ì§€ ëª»í•˜ë©°, ì†ì‹¤ì´ ë†’ìŒ
  - **í•™ìŠµ ê°€ëŠ¥í•œ Predictor**:  
    - ë™ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ëŠ” $h$ëŠ” SimSiamì˜ ì•ˆì •ì  í•™ìŠµì„ ë„ì›€

---

### 4.3 Batch Size

- **íš¨ê³¼**:  
  - SimSiamì€ ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸°(64~4096)ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ì‘ë™
  - ë°°ì¹˜ í¬ê¸°ê°€ ì‘ì•„ë„ ì„±ëŠ¥ ì €í•˜ê°€ ì—†ìŒ

- **SimCLR ë° SwAVì™€ì˜ ì°¨ì´ì **:  
  - SimCLRì™€ SwAVëŠ” í° ë°°ì¹˜ í¬ê¸°(4096 ì´ìƒ)ê°€ í•„ìš”í•˜ì§€ë§Œ, SimSiamì€ ì¼ë°˜ì ì¸ í¬ê¸°(512)ì—ì„œë„ ì‘ë™

---

### 4.4 Batch Normalization

- **íš¨ê³¼**:  
  - Batch Normalization(BN)ì€ ê° ë ˆì´ì–´ì˜ ì¶œë ¥ ë¶„í¬ë¥¼ ì•ˆì •í™”í•˜ì—¬ ìµœì í™”
  - SimSiamì—ì„œ BNì€ collapsing ë°©ì§€ì—ëŠ” ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠìŒ  

- **êµ¬ì„±**:  
  - **Projection MLP**:  
    - BNì€ ëª¨ë“  FC ë ˆì´ì–´ì— ì ìš©ë˜ë©°, ì¶œë ¥ FCì—ëŠ” ReLUê°€ ì—†ìŒ
  - **Prediction MLP**:  
    - BNì€ ì€ë‹‰ FC ë ˆì´ì–´ì— ì ìš©ë˜ì§€ë§Œ, ì¶œë ¥ FCì—ëŠ” ì ìš©ë˜ì§€ ì•ŠìŒ

---

### 4.5 Symmetrization

- **ìˆ˜ì‹**:  
  $$L = \frac{1}{2} D(p_1, stopgrad(z_2)) + \frac{1}{2} D(p_2, stopgrad(z_1))$$  
  - ë‘ augmentation ê°„ ì†ì‹¤ì„ ëŒ€ì¹­ì ìœ¼ë¡œ ê³„ì‚°í•˜ì—¬ í•™ìŠµ ì•ˆì •ì„±ì„ ë†’ì„

- **íš¨ê³¼**:  
  - ëŒ€ì¹­í™”ëŠ” ì •í™•ë„ë¥¼ ë†’ì´ëŠ” ë° ê¸°ì—¬í•˜ì§€ë§Œ, collapsing ë°©ì§€ì—ëŠ” í•„ìˆ˜ì ì´ì§€ ì•ŠìŒ

- **ë¹„êµ ì‹¤í—˜**:  
  - ëŒ€ì¹­í™” ì œê±°(ë¹„ëŒ€ì¹­ ì†ì‹¤)ë¥¼ ì ìš©í•´ë„ ëª¨ë¸ì´ collapsing ì—†ì´ í•™ìŠµí•  ìˆ˜ ìˆì§€ë§Œ, ì •í™•ë„ëŠ” ì•½ê°„ ê°ì†Œ
