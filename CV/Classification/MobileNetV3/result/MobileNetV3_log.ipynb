{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/PARKYUNSU/MobileNetV3","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-16T01:35:34.766695Z","iopub.execute_input":"2024-12-16T01:35:34.767012Z","iopub.status.idle":"2024-12-16T01:35:36.199455Z","shell.execute_reply.started":"2024-12-16T01:35:34.766983Z","shell.execute_reply":"2024-12-16T01:35:36.198647Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'MobileNetV3'...\nremote: Enumerating objects: 80, done.\u001b[K\nremote: Counting objects: 100% (80/80), done.\u001b[K\nremote: Compressing objects: 100% (51/51), done.\u001b[K\nremote: Total 80 (delta 41), reused 66 (delta 27), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (80/80), 62.49 KiB | 4.81 MiB/s, done.\nResolving deltas: 100% (41/41), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!python \"/kaggle/working/MobileNetV3/main.py\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T01:35:36.201542Z","iopub.execute_input":"2024-12-16T01:35:36.202192Z","iopub.status.idle":"2024-12-16T02:37:21.308166Z","shell.execute_reply.started":"2024-12-16T01:35:36.202151Z","shell.execute_reply":"2024-12-16T02:37:21.307117Z"}},"outputs":[{"name":"stdout","text":"====================================================================================================\nLayer (type:depth-idx)                             Output Shape              Param #\n====================================================================================================\nMobilenetV3                                        [2, 1000]                 --\n├─Sequential: 1-1                                  [2, 16, 112, 112]         --\n│    └─Conv2d: 2-1                                 [2, 16, 112, 112]         432\n│    └─BatchNorm2d: 2-2                            [2, 16, 112, 112]         32\n│    └─Hardswish: 2-3                              [2, 16, 112, 112]         --\n├─Sequential: 1-2                                  [2, 160, 7, 7]            --\n│    └─InvertedResBlock: 2-4                       [2, 16, 112, 112]         288\n│    │    └─Sequential: 3-1                        [2, 16, 112, 112]         464\n│    └─InvertedResBlock: 2-5                       [2, 24, 56, 56]           --\n│    │    └─Sequential: 3-2                        [2, 24, 56, 56]           3,440\n│    └─InvertedResBlock: 2-6                       [2, 24, 56, 56]           --\n│    │    └─Sequential: 3-3                        [2, 24, 56, 56]           4,440\n│    └─InvertedResBlock: 2-7                       [2, 40, 28, 28]           --\n│    │    └─Sequential: 3-4                        [2, 40, 28, 28]           9,458\n│    └─InvertedResBlock: 2-8                       [2, 40, 28, 28]           --\n│    │    └─Sequential: 3-5                        [2, 40, 28, 28]           20,510\n│    └─InvertedResBlock: 2-9                       [2, 40, 28, 28]           --\n│    │    └─Sequential: 3-6                        [2, 40, 28, 28]           20,510\n│    └─InvertedResBlock: 2-10                      [2, 80, 14, 14]           --\n│    │    └─Sequential: 3-7                        [2, 80, 14, 14]           32,080\n│    └─InvertedResBlock: 2-11                      [2, 80, 14, 14]           --\n│    │    └─Sequential: 3-8                        [2, 80, 14, 14]           34,760\n│    └─InvertedResBlock: 2-12                      [2, 80, 14, 14]           --\n│    │    └─Sequential: 3-9                        [2, 80, 14, 14]           31,992\n│    └─InvertedResBlock: 2-13                      [2, 80, 14, 14]           --\n│    │    └─Sequential: 3-10                       [2, 80, 14, 14]           31,992\n│    └─InvertedResBlock: 2-14                      [2, 112, 14, 14]          --\n│    │    └─Sequential: 3-11                       [2, 112, 14, 14]          214,424\n│    └─InvertedResBlock: 2-15                      [2, 112, 14, 14]          --\n│    │    └─Sequential: 3-12                       [2, 112, 14, 14]          386,120\n│    └─InvertedResBlock: 2-16                      [2, 160, 7, 7]            --\n│    │    └─Sequential: 3-13                       [2, 160, 7, 7]            429,224\n│    └─InvertedResBlock: 2-17                      [2, 160, 7, 7]            --\n│    │    └─Sequential: 3-14                       [2, 160, 7, 7]            797,360\n│    └─InvertedResBlock: 2-18                      [2, 160, 7, 7]            --\n│    │    └─Sequential: 3-15                       [2, 160, 7, 7]            797,360\n├─Sequential: 1-3                                  [2, 960, 7, 7]            --\n│    └─Conv2d: 2-19                                [2, 960, 7, 7]            153,600\n│    └─BatchNorm2d: 2-20                           [2, 960, 7, 7]            1,920\n│    └─Hardswish: 2-21                             [2, 960, 7, 7]            --\n├─AdaptiveAvgPool2d: 1-4                           [2, 960, 1, 1]            --\n├─Sequential: 1-5                                  [2, 1280]                 --\n│    └─Linear: 2-22                                [2, 1280]                 1,230,080\n│    └─Hardswish: 2-23                             [2, 1280]                 --\n├─Sequential: 1-6                                  [2, 1000]                 --\n│    └─Dropout: 2-24                               [2, 1280]                 --\n│    └─Linear: 2-25                                [2, 1000]                 1,281,000\n====================================================================================================\nTotal params: 5,481,486\nTrainable params: 5,481,486\nNon-trainable params: 0\nTotal mult-adds (M): 433.24\n====================================================================================================\nInput size (MB): 1.20\nForward/backward pass size (MB): 140.91\nParams size (MB): 21.92\nEstimated Total Size (MB): 164.04\n====================================================================================================\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n100%|███████████████████████| 170498071/170498071 [00:02<00:00, 77708849.78it/s]\nExtracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\nDataset: CIFAR-10\nBatch Size: 128, Learning Rate: 0.001, Weight Decay: 0.0001\n\nEpoch 1/30\nTrain Loss: 1.3266, Train Acc: 51.02%                                           \nVal Loss: 1.0472, Val Acc: 62.51%\nBest model saved!\n\nEpoch 2/30\nTrain Loss: 0.8426, Train Acc: 70.05%                                           \nVal Loss: 0.7941, Val Acc: 72.52%\nBest model saved!\n\nEpoch 3/30\nTrain Loss: 0.6571, Train Acc: 76.98%                                           \nVal Loss: 0.6383, Val Acc: 77.84%\nBest model saved!\n\nEpoch 4/30\nTrain Loss: 0.5458, Train Acc: 80.93%                                           \nVal Loss: 0.6225, Val Acc: 78.08%\nBest model saved!\n\nEpoch 5/30\nTrain Loss: 0.4720, Train Acc: 83.62%                                           \nVal Loss: 0.5746, Val Acc: 80.56%\nBest model saved!\n\nEpoch 6/30\nTrain Loss: 0.4132, Train Acc: 85.81%                                           \nVal Loss: 0.5772, Val Acc: 80.39%\n\nEpoch 7/30\nTrain Loss: 0.3691, Train Acc: 87.19%                                           \nVal Loss: 0.5285, Val Acc: 82.16%\nBest model saved!\n\nEpoch 8/30\nTrain Loss: 0.3263, Train Acc: 88.69%                                           \nVal Loss: 0.5692, Val Acc: 82.00%\n\nEpoch 9/30\nTrain Loss: 0.2936, Train Acc: 89.84%                                           \nVal Loss: 0.5381, Val Acc: 82.86%\nBest model saved!\n\nEpoch 10/30\nTrain Loss: 0.2637, Train Acc: 90.84%                                           \nVal Loss: 0.5169, Val Acc: 83.33%\nBest model saved!\n\nEpoch 11/30\nTrain Loss: 0.2337, Train Acc: 91.80%                                           \nVal Loss: 0.5606, Val Acc: 82.80%\n\nEpoch 12/30\nTrain Loss: 0.2141, Train Acc: 92.60%                                           \nVal Loss: 0.5041, Val Acc: 83.97%\nBest model saved!\n\nEpoch 13/30\nTrain Loss: 0.1984, Train Acc: 92.97%                                           \nVal Loss: 0.5147, Val Acc: 84.89%\nBest model saved!\n\nEpoch 14/30\nTrain Loss: 0.1815, Train Acc: 93.56%                                           \nVal Loss: 0.5632, Val Acc: 83.78%\n\nEpoch 15/30\nTrain Loss: 0.1649, Train Acc: 94.17%                                           \nVal Loss: 0.5632, Val Acc: 84.09%\n\nEpoch 16/30\nTrain Loss: 0.1523, Train Acc: 94.58%                                           \nVal Loss: 0.5281, Val Acc: 84.27%\n\nEpoch 17/30\nTrain Loss: 0.1413, Train Acc: 95.02%                                           \nVal Loss: 0.5873, Val Acc: 83.89%\n\nEpoch 18/30\nTrain Loss: 0.1359, Train Acc: 95.25%                                           \nVal Loss: 0.5646, Val Acc: 84.65%\n\nEpoch 19/30\nTrain Loss: 0.1264, Train Acc: 95.59%                                           \nVal Loss: 0.6318, Val Acc: 83.07%\n\nEpoch 20/30\nTrain Loss: 0.1159, Train Acc: 95.96%                                           \nVal Loss: 0.6299, Val Acc: 83.63%\n\nEpoch 21/30\nTrain Loss: 0.1173, Train Acc: 95.82%                                           \nVal Loss: 0.5832, Val Acc: 85.19%\nBest model saved!\n\nEpoch 22/30\nTrain Loss: 0.1062, Train Acc: 96.30%                                           \nVal Loss: 0.6023, Val Acc: 84.77%\n\nEpoch 23/30\nTrain Loss: 0.1045, Train Acc: 96.38%                                           \nVal Loss: 0.6511, Val Acc: 83.93%\n\nEpoch 24/30\nTrain Loss: 0.1016, Train Acc: 96.47%                                           \nVal Loss: 0.6367, Val Acc: 84.23%\n\nEpoch 25/30\nTrain Loss: 0.0934, Train Acc: 96.71%                                           \nVal Loss: 0.5844, Val Acc: 84.81%\n\nEpoch 26/30\nTrain Loss: 0.0948, Train Acc: 96.74%                                           \nVal Loss: 0.6160, Val Acc: 84.20%\n\nEpoch 27/30\nTrain Loss: 0.0889, Train Acc: 96.94%                                           \nVal Loss: 0.6536, Val Acc: 83.69%\n\nEpoch 28/30\nTrain Loss: 0.0902, Train Acc: 96.83%                                           \nVal Loss: 0.6401, Val Acc: 83.41%\n\nEpoch 29/30\nTrain Loss: 0.0873, Train Acc: 96.98%                                           \nVal Loss: 0.5477, Val Acc: 85.36%\nBest model saved!\n\nEpoch 30/30\nTrain Loss: 0.0765, Train Acc: 97.35%                                           \nVal Loss: 0.6058, Val Acc: 84.69%\n","output_type":"stream"}],"execution_count":2}]}